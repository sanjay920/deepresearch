Generate a mermaid diagram starting from the entrypoint cli.py for a user request:
```
Create a 50-page research report on AI agent platforms with the following requirements: 1. Model after a typical research analyst report from Gartner or Wall Street investment analyst firms. 2. Perform research on up to 1000 publicly available websites and documents. 3. Include figures, tables, and charts in the report when appropriate. 4. Include a survey of products and solutions on the market. 5. Compare feature sets, customer traction, user adoption, and projected revenue of these products. 6. Include software, services, open source, and SaaS products. 7. Scrape blogs, tweets, and YouTube videos to collect actual user experience and feedback about these products. 8. Include links and citations to all third-party web pages, documents, tweets, and YouTube videos
```

The one i previously had was this, but since then the code has been updated a lot:
sequenceDiagram
    participant U as User
    participant CLI as cli.py
    participant Convo as conversation.py
    participant LLM as llm_client.py
    participant TD as tool_dispatcher.py
    participant O as orchestrator.py
    participant R as retrieval.py
    participant G as google_search.py
    participant S as synthesis.py
    participant V as validation.py

    U->>CLI: "Find NBA players over age 30 with >10 ppg"
    CLI->>Convo: Store user message in conversation
    CLI->>LLM: Send entire conversation to the LLM
    Note over LLM: LLM may decide to call a tool<br/>to gather NBA stats
    LLM->>TD: Tool call for <br/>google_search(q="NBA players over 30 averaging >10 ppg")
    TD->>O: Register a retrieval task
    O->>R: Calls RetrievalAgent
    R->>G: google_search_call(query)
    G->>R: JSON results returned
    R->>O: Return data from retrieval
    Note over O: Once retrieval completes,<br/>Orchestrator dispatches synthesis
    O->>S: Calls SynthesisAgent <br/>to summarize the found data
    S->>LLM: Summarize the data <br/>via an LLM call
    LLM->>S: Returns a summary
    S->>O: Provide summarized text
    Note over O: Orchestrator may call <br/>ValidationAgent if needed
    O->>V: Validate the summary <br/>against sources
    V->>LLM: Compare summary vs. source data
    LLM->>V: Returns validation report
    V->>O: Pass final validated summary
    O->>CLI: Final answer or summary
    CLI->>U: Return answer to user


================================================================
Directory Structure
================================================================
tools/
  google_search.py
  scrape_urls.py
  switch_personas.py
cli.py
config.py
conversation.py
llm_client.py
orchestrator.py
retrieval.py
switch_personas.md
synthesis.py
tool_dispatcher.py
utils.py
validation.py

================================================================
Files
================================================================

================
File: tools/google_search.py
================
import requests


def google_search_call(q):
    """
    Calls the Google Custom Search API microservice at:
      http://localhost:8085/search

    It sends the search query 'q' as a GET parameter and returns the filtered JSON response.
    """
    api_url = "http://localhost:8085/search"
    params = {"q": q}
    response = requests.get(api_url, params=params)
    response.raise_for_status()
    return response.json()

================
File: tools/scrape_urls.py
================
import requests


def scrape_urls_call(urls, use_cache=True):
    """
    Calls the Firecrawl service by making a POST request to:
      http://localhost:8084/batch_scrape_urls

    Parameters:
    - urls: List of URLs to scrape
    - use_cache: When True, uses cached results if available. When False, forces fresh fetches.
    """
    api_url = "http://localhost:8084/batch_scrape_urls"
    payload = {
        "urls": urls,
        "formats": ["markdown"],
        "force_fetch": not use_cache,  # When use_cache is True, we do not force fetch
    }

    print(payload)

    response = requests.post(api_url, json=payload)
    response.raise_for_status()
    return response.json()

================
File: tools/switch_personas.py
================
import json
from openai import OpenAI
import os
from datetime import datetime
from conversation import Conversation  # Explicitly import Conversation class

# Initialize OpenAI client with API key from environment variables
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# System prompt instructing the thinker persona on how to respond
SYSTEM_PROMPT = f"""Formatting re-enabled
You are the assistant's subconscious, tasked with planning and thinking to address the user's request. You have access to the following tools:

Tools:
[
    {{
        "type": "function",
        "function": {{
            "name": "scrape_urls",
            "strict": True,
            "parameters": {{
                "type": "object",
                "required": ["urls", "use_cache"],
                "properties": {{
                    "urls": {{
                        "type": "array",
                        "items": {{"type": "string"}},
                        "description": "An array of URLs to scrape"
                    }},
                    "use_cache": {{
                        "type": "boolean",
                        "description": "Whether to use cached data (default: true). Set to false for dynamic content."
                    }}
                }},
                "additionalProperties": False
            }},
            "description": "Scrapes webpages and returns their markdown equivalent"
        }}
    }},
    {{
        "type": "function",
        "function": {{
            "name": "google_search",
            "parameters": {{
                "type": "object",
                "required": ["q"],
                "properties": {{
                    "q": {{"type": "string", "description": "The search query string"}}
                }},
                "additionalProperties": False
            }},
            "description": "Performs a Google search and returns results"
        }}
    }}
]

**Instructions:**
- Review the conversation history provided below.
- For multi-step or data-intensive requests (e.g., research reports), set 'action' to 'plan' and:
  - Provide a 'text' field with a natural language description of the plan in Markdown.
  - Include a 'steps' field listing specific tool calls with their parameters (e.g., google_search, scrape_urls).
  - Example:
    ```json
    {{
        "action": "plan",
        "text": "## Research Plan\\nSearch for AI agent platform reports and scrape relevant sites.",
        "steps": [
            {{"tool": "google_search", "parameters": {{"q": "AI agent platforms research report"}}}},
            {{"tool": "scrape_urls", "parameters": {{"urls": ["https://example.com"], "use_cache": true}}}}
        ],
        "respond_to_user": false
    }}
    ```
- For unclear or incomplete requests, set 'action' to 'questions', provide questions in 'text' in Markdown, and set 'respond_to_user' to false.
- For simple queries (e.g., schedules), propose a concise plan with a single step if sufficient.
- Do not use scratchpad tools here; let the agent handle data storage.
- Today's date is {datetime.now().strftime('%B %d, %Y')}. Use this for time-sensitive queries.
- Keep plans or questions concise and directly aligned with the user's request.
"""


def switch_personas_call(
    switch_to: str, conversation: Conversation, thinking_level: str = "high"
) -> str:
    """
    Switches the assistant to the 'thinker' persona to generate a plan or questions.

    Args:
        switch_to (str): The persona to switch to (must be "thinker").
        conversation (Conversation): The current conversation context, an instance of the Conversation class.
        thinking_level (str): Level of thinking effort ("high", "medium", "low").

    Returns:
        str: JSON string containing the thinker's response (plan or questions).

    Raises:
        ValueError: If switch_to is not "thinker".
    """
    if switch_to != "thinker":
        raise ValueError("Only switching to 'thinker' is supported.")

    # Get conversation history for context using the Conversation class method
    conversation_history = conversation.format_conversation()
    user_message = f"Conversation history:\n{conversation_history}"

    # Define the corrected response format schema based on your example
    response_format = {
        "type": "json_schema",
        "json_schema": {
            "name": "thinker_response",
            "strict": False,  # Relaxed strictness to avoid overly restrictive validation
            "schema": {
                "type": "object",
                "required": ["action", "text", "respond_to_user"],
                "properties": {
                    "action": {
                        "enum": ["plan", "questions"],
                        "type": "string",
                        "description": "Type of response: 'plan' for a detailed plan, 'questions' for clarifications.",
                    },
                    "text": {
                        "type": "string",
                        "description": "Description of the plan or questions to ask, in Markdown.",
                    },
                    "steps": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "oneOf": [
                                {
                                    "properties": {
                                        "tool": {
                                            "const": "google_search",
                                            "type": "string",
                                            "description": "Tool to perform a Google search.",
                                        },
                                        "parameters": {
                                            "type": "object",
                                            "required": ["q"],
                                            "properties": {
                                                "q": {
                                                    "type": "string",
                                                    "description": "The search query string.",
                                                }
                                            },
                                            "additionalProperties": False,
                                        },
                                    },
                                    "required": ["tool", "parameters"],
                                    "additionalProperties": False,
                                },
                                {
                                    "properties": {
                                        "tool": {
                                            "const": "scrape_urls",
                                            "type": "string",
                                            "description": "Tool to scrape webpages.",
                                        },
                                        "parameters": {
                                            "type": "object",
                                            "required": ["urls", "use_cache"],
                                            "properties": {
                                                "urls": {
                                                    "type": "array",
                                                    "items": {"type": "string"},
                                                    "description": "An array of URLs to scrape.",
                                                },
                                                "use_cache": {
                                                    "type": "boolean",
                                                    "description": "Whether to use cached data (default: true).",
                                                },
                                            },
                                            "additionalProperties": False,
                                        },
                                    },
                                    "required": ["tool", "parameters"],
                                    "additionalProperties": False,
                                },
                            ],
                            "description": "A step in the plan specifying a tool and its parameters.",
                        },
                        "description": "List of tool calls for the plan. Include when action is 'plan'.",
                    },
                    "respond_to_user": {
                        "type": "boolean",
                        "description": "Whether this response is for the user (false by default).",
                    },
                },
                "additionalProperties": False,
            },
        },
    }

    try:
        # Call the OpenAI API with system prompt as 'developer' message and user message with history
        response = client.chat.completions.create(
            model="o3-mini",
            messages=[
                {"role": "developer", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_message},
            ],
            response_format=response_format,
            reasoning_effort=thinking_level,
            max_completion_tokens=5000,  # Increased to handle complex report planning
        )
        return response.choices[0].message.content
    except Exception as e:
        error_msg = f"Failed to switch personas: {str(e)}"
        return json.dumps({"error": error_msg})

================
File: cli.py
================
#!/usr/bin/env python3
import json
import click
import logging
from rich.console import Console
from rich.markdown import Markdown
from threading import Thread
from datetime import datetime
import tiktoken

from conversation import Conversation
from llm_client import stream_message
from tool_dispatcher import dispatch_tool_call
from orchestrator import Orchestrator, Task
from utils import setup_logging

console = Console()


class ToolCallAggregator:
    def __init__(self):
        self.final_tool_calls = {}

    def reset(self):
        """Reset the aggregator for a new response."""
        self.final_tool_calls = {}

    def process_delta(self, tool_calls):
        """Process a delta chunk of tool calls."""
        if not tool_calls:
            return
        calls = tool_calls if isinstance(tool_calls, list) else [tool_calls]
        for tool_call in calls:
            index = tool_call.index
            if index not in self.final_tool_calls:
                self.final_tool_calls[index] = {
                    "index": index,
                    "type": "function",
                    "function": {"name": "", "arguments": ""},
                    "id": tool_call.id if tool_call.id else None,
                }
            if tool_call.id and self.final_tool_calls[index]["id"] is None:
                self.final_tool_calls[index]["id"] = tool_call.id
            if tool_call.function:
                if tool_call.function.name:
                    self.final_tool_calls[index]["function"][
                        "name"
                    ] = tool_call.function.name
                if tool_call.function.arguments:
                    self.final_tool_calls[index]["function"][
                        "arguments"
                    ] += tool_call.function.arguments

    def get_all_calls(self):
        """Return all aggregated tool calls."""
        return list(self.final_tool_calls.values())

    def finalize(self):
        """Finalize the aggregation process (currently a no-op)."""
        pass


@click.command()
@click.option("--debug", is_flag=True, help="Enable debug mode for detailed logging")
@click.option("--show-validation", is_flag=True, help="Show detailed validation report")
def chat_cli(debug: bool, show_validation: bool):
    """CLI interface for interacting with the assistant and generating reports."""
    setup_logging(verbose=debug)

    system_message = {
        "role": "system",
        "content": [
            {
                "type": "text",
                "text": (
                    "You are an assistant wearing your 'agent' hat. Your goal is to fully answer the user's query by gathering and processing data. You can:\n"
                    "- Use 'google_search' to find relevant sources.\n"
                    "- Use 'scrape_urls' to fetch full content from specific websites when search snippets aren’t enough.\n"
                    "- Iterate by adding more searches or scrapes if initial data is incomplete.\n"
                    "- Use scratchpad tools to store intermediate data if needed.\n\n"
                    "For queries requiring specific details (e.g., lists, ages), follow this flow:\n"
                    "1. Start with a broad search to identify sources.\n"
                    "2. Scrape relevant websites to get detailed data.\n"
                    "3. If data is missing, queue more searches or scrapes to fill gaps.\n"
                    "4. Synthesize and validate the final answer with all collected data.\n\n"
                    "Don’t suggest users visit links—fetch the data yourself and provide a complete answer. "
                    "For complex tasks, call 'switch_personas' with 'thinker' to plan multi-step processes. "
                    f"Today’s date is {datetime.now().strftime('%B %d, %Y')}. Calculate ages based on birth dates relative to this date."
                ),
            }
        ],
    }

    conversation = Conversation()
    conversation.add_message(system_message)
    tool_aggregator = ToolCallAggregator()
    orchestrator = Orchestrator()
    scratchpad_state = {}  # Initialize persistent scratchpad state

    while True:
        user_input = console.input("[bold yellow]You:[/] ")
        if user_input.lower() in ("exit", "quit"):
            console.print("[bold red]Goodbye![/]")
            break

        conversation.add_message(
            {"role": "user", "content": [{"type": "text", "text": user_input}]}
        )

        console.print("[dim]Currently processing: Handling your request...[/]")

        try:
            while True:
                full_response = ""
                tool_aggregator.reset()

                stream = stream_message(conversation.get_history())
                for chunk in stream:
                    if chunk.choices[0].delta.content:
                        full_response += chunk.choices[0].delta.content
                    if chunk.choices[0].delta.tool_calls:
                        tool_aggregator.process_delta(chunk.choices[0].delta.tool_calls)
                tool_calls = tool_aggregator.get_all_calls()

                if not tool_calls:
                    console.print("\n[bold blue]Final Answer:[/]")
                    console.print(Markdown(full_response))
                    break

                assistant_message = {
                    "role": "assistant",
                    "content": [],
                    "tool_calls": [
                        {
                            "id": tc["id"],
                            "type": "function",
                            "function": {
                                "name": tc["function"]["name"],
                                "arguments": tc["function"]["arguments"],
                            },
                        }
                        for tc in tool_calls
                    ],
                }
                conversation.add_message(assistant_message)

                for tc in tool_calls:
                    try:
                        raw_args = tc["function"]["arguments"]
                        if not raw_args or raw_args.isspace():
                            continue
                        args_obj = json.loads(raw_args)
                        console.print(
                            f"[dim]Currently processing: {tc['function']['name']} call[/]"
                        )
                        result = dispatch_tool_call(
                            {
                                "id": tc["id"],
                                "function": {
                                    "name": tc["function"]["name"],
                                    "arguments": args_obj,
                                },
                            },
                            conversation=conversation,
                            scratchpad_state=scratchpad_state,
                        )
                        if result is None:
                            result = "Tool call returned no result."
                        tool_message = {
                            "role": "tool",
                            "content": [{"type": "text", "text": str(result)}],
                            "tool_call_id": tc["id"],
                        }
                        conversation.add_message(tool_message)

                        if tc["function"]["name"] == "switch_personas":
                            thinker_response = json.loads(result)
                            if "error" in thinker_response:
                                console.print(
                                    f"[bold red]Thinker Error:[/] {thinker_response['error']}"
                                )
                                break
                            if thinker_response["action"] == "plan":
                                console.print(
                                    "[dim]Currently processing: Executing thinker’s plan[/]"
                                )
                                for step in thinker_response.get("steps", []):
                                    if step["tool"] == "google_search":
                                        task_id = f"retrieval_{tc['id']}_{step['parameters']['q']}"
                                        task = Task(
                                            task_id=task_id,
                                            task_type="retrieval",
                                            params={"query": step["parameters"]["q"]},
                                        )
                                        orchestrator.add_task(task)
                                    elif step["tool"] == "scrape_urls":
                                        for url in step["parameters"]["urls"]:
                                            task_id = f"retrieval_url_{tc['id']}_{url}"
                                            task = Task(
                                                task_id=task_id,
                                                task_type="retrieval",
                                                params={"url": url},
                                            )
                                            orchestrator.add_task(task)
                            elif thinker_response["action"] == "questions":
                                console.print("\n[bold yellow]Questions for you:[/]")
                                console.print(Markdown(thinker_response["text"]))
                                user_response = console.input(
                                    "[bold yellow]Your response:[/] "
                                )
                                conversation.add_message(
                                    {
                                        "role": "user",
                                        "content": [
                                            {"type": "text", "text": user_response}
                                        ],
                                    }
                                )
                        elif tc["function"]["name"] == "google_search":
                            task_id = f"retrieval_{tc['id']}"
                            task = Task(
                                task_id=task_id,
                                task_type="retrieval",
                                params={"query": args_obj["q"]},
                            )
                            orchestrator.add_task(task)
                        elif tc["function"]["name"] == "scrape_urls":
                            for url in args_obj["urls"]:
                                task_id = f"retrieval_url_{tc['id']}_{url}"
                                task = Task(
                                    task_id=task_id,
                                    task_type="retrieval",
                                    params={"url": url},
                                )
                                orchestrator.add_task(task)
                    except Exception as ex:
                        logging.error(f"Tool call error: {ex}")
                        console.print(f"[bold red]Error in tool call:[/] {str(ex)}")
                        conversation.add_message(
                            {
                                "role": "tool",
                                "content": [
                                    {"type": "text", "text": f"Error: {str(ex)}"}
                                ],
                                "tool_call_id": tc["id"],
                            }
                        )

                if orchestrator.get_pending_tasks():
                    retrieval_task_ids = [
                        task.task_id
                        for task in orchestrator.tasks.values()
                        if task.task_type == "retrieval"
                    ]
                    for retrieval_task_id in retrieval_task_ids:
                        synthesis_task = Task(
                            task_id=f"synthesis_{retrieval_task_id}",
                            task_type="synthesis",
                            params={
                                "query": user_input,
                            },
                            dependencies=[retrieval_task_id],
                        )
                        orchestrator.add_task(synthesis_task)
                        validation_task = Task(
                            task_id=f"validation_{synthesis_task.task_id}",
                            task_type="validation",
                            params={
                                "synthesis_task_id": synthesis_task.task_id,
                            },
                            dependencies=[synthesis_task.task_id],
                        )
                        orchestrator.add_task(validation_task)

                    console.print(
                        "[dim]Currently processing: Executing queued tasks[/]"
                    )

                    def task_start_callback(task):
                        console.print(
                            f"[dim]Currently processing: {task.get_description()}[/]"
                        )

                    thread = Thread(
                        target=orchestrator.execute_tasks,
                        kwargs={"on_task_start": task_start_callback},
                    )
                    thread.start()
                    thread.join()

                    console.print("[dim]Task execution summary:[/]")
                    for task_id, task in orchestrator.tasks.items():
                        status = task.status
                        description = task.get_description()
                        console.print(f"{task_id}: {status} - {description}")

                    synthesis_tasks = [
                        task
                        for task in orchestrator.tasks.values()
                        if task.task_type == "synthesis" and task.status == "completed"
                    ]
                    if synthesis_tasks:
                        final_synthesis = synthesis_tasks[-1].result
                        synthesis_output = json.loads(final_synthesis)
                        if synthesis_output["status"] == "complete":
                            console.print("\n[bold blue]Final Answer:[/]")
                            console.print(Markdown(synthesis_output["summary"]))
                        else:
                            console.print(
                                "\n[bold yellow]Partial Answer (Max Iterations Reached):[/]"
                            )
                            console.print(Markdown(synthesis_output["summary"]))
                        if show_validation:
                            validation_tasks = [
                                task
                                for task in orchestrator.tasks.values()
                                if task.task_type == "validation"
                                and task.status == "completed"
                            ]
                            if validation_tasks:
                                console.print("\n[bold green]Validation Report:[/]")
                                console.print(Markdown(validation_tasks[-1].result))
                    else:
                        failed_retrievals = [
                            task
                            for task in orchestrator.tasks.values()
                            if task.task_type == "retrieval" and task.status == "failed"
                        ]
                        if failed_retrievals:
                            console.print(
                                "[bold red]Error:[/] Some retrieval tasks failed, preventing synthesis."
                            )
                            for task in failed_retrievals:
                                console.print(f"- Failed: {task.get_description()}")
                        else:
                            console.print(
                                "[bold red]Error:[/] No synthesis results available."
                            )
                    break

        except Exception as e:
            console.print(f"\n[bold red]Error:[/] {str(e)}")
            logging.error(f"CLI error: {e}")


if __name__ == "__main__":
    chat_cli()

================
File: config.py
================
import os

config = {"openai_api_key": os.getenv("OPENAI_API_KEY"), "model": "gpt-4o"}

================
File: conversation.py
================
class Conversation:
    """A simple in-memory conversation history manager."""

    def __init__(self):
        self.history = []

    def add_message(self, message):
        self.history.append(message)

    def get_history(self):
        return self.history

    def format_conversation(self):
        """
        Formats the conversation history into a string format suitable for the thinker persona.
        """
        formatted = []
        for msg in self.history:
            role = msg.get("role", "")

            if role == "assistant" and msg.get("tool_calls"):
                # Format tool calls
                tool_calls = []
                for call in msg["tool_calls"]:
                    tool_name = call["function"]["name"]
                    tool_calls.append(f"[Tool Call] {tool_name}")
                formatted.append(f"assistant: {' '.join(tool_calls)}")

            elif role == "tool":
                # Format tool responses
                content = msg.get("content", [{}])[0].get("text", "")
                formatted.append(f"tool: {content}")

            elif role == "user":
                # Format user messages
                content = msg.get("content", [{}])[0].get("text", "")
                formatted.append(f"user: {content}")

            elif role == "assistant" and msg.get("content"):
                # Format assistant responses
                content = msg.get("content", [{}])[0].get("text", "")
                formatted.append(f"assistant: {content}")

        return "\n\n".join(formatted)

    def get_history_excluding_scratchpad_msgs(self):
        # For now, we return the full history so past tool calls remain visible.
        return self.history

================
File: llm_client.py
================
import os
from openai import OpenAI

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Tools that DO include scratchpad calls
TOOLS_WITH_SCRATCHPAD = [
    {
        "type": "function",
        "function": {
            "name": "scrape_urls",
            "strict": True,
            "parameters": {
                "type": "object",
                "required": ["urls", "use_cache"],
                "properties": {
                    "urls": {
                        "type": "array",
                        "items": {
                            "type": "string",
                            "description": "A single URL of a webpage",
                        },
                        "description": "An array of string URLs of the webpages to scrape",
                    },
                    "use_cache": {
                        "type": "boolean",
                        "description": "Flag to indicate whether to use cached data",
                    },
                },
                "additionalProperties": False,
            },
            "description": "Scrapes a webpage and returns its markdown equivalent",
        },
    },
    {
        "type": "function",
        "function": {
            "name": "switch_personas",
            "strict": True,
            "parameters": {
                "type": "object",
                "required": ["switch_to", "justification", "thinking_level"],
                "properties": {
                    "switch_to": {
                        "enum": ["thinker"],
                        "type": "string",
                        "description": "The persona to switch to",
                    },
                    "justification": {
                        "type": "string",
                        "description": "Reason for switching persona",
                    },
                    "thinking_level": {
                        "enum": ["high", "medium", "low"],
                        "type": "string",
                        "description": "Amount of thinking done by the thinker persona",
                    },
                },
                "additionalProperties": False,
            },
            "description": "Ability to switch personas in the middle of the conversation",
        },
    },
    {
        "type": "function",
        "function": {
            "name": "google_search",
            "strict": True,
            "parameters": {
                "type": "object",
                "required": ["q"],
                "properties": {
                    "q": {"type": "string", "description": "The search query string."}
                },
                "additionalProperties": False,
            },
            "description": "Performs a Google search with the specified query and returns results.",
        },
    },
    # ------------------------------
    # Scratchpad Tools
    # ------------------------------
    {
        "type": "function",
        "function": {
            "name": "scratchpad_create_entry",
            "description": "Create or append a note in the agent’s scratchpad. MLA citations encouraged.",
            "parameters": {
                "type": "object",
                "required": ["title", "content"],
                "properties": {
                    "title": {
                        "type": "string",
                        "description": "Short descriptive title for the scratchpad entry.",
                    },
                    "content": {
                        "type": "string",
                        "description": "Note content (text). Include MLA citations if referencing external sources.",
                    },
                },
                "additionalProperties": False,
            },
            "strict": True,
        },
    },
    {
        "type": "function",
        "function": {
            "name": "scratchpad_delete_entry",
            "description": "Deletes a note/block from the scratchpad by ID number.",
            "parameters": {
                "type": "object",
                "required": ["entry_id"],
                "properties": {
                    "entry_id": {
                        "type": "integer",
                        "description": "Scratchpad note ID to delete",
                    }
                },
                "additionalProperties": False,
            },
            "strict": True,
        },
    },
    {
        "type": "function",
        "function": {
            "name": "scratchpad_replace_entry",
            "description": "Replaces the content of a scratchpad note by ID.",
            "parameters": {
                "type": "object",
                "required": ["entry_id", "content"],
                "properties": {
                    "entry_id": {
                        "type": "integer",
                        "description": "Scratchpad note ID to modify",
                    },
                    "content": {
                        "type": "string",
                        "description": "New content. Use MLA style citations if referencing external data.",
                    },
                },
                "additionalProperties": False,
            },
            "strict": True,
        },
    },
]

# Tools that EXCLUDE scratchpad calls
TOOLS_NO_SCRATCHPAD = [
    {
        "type": "function",
        "function": {
            "name": "scrape_urls",
            "strict": True,
            "parameters": {
                "type": "object",
                "required": ["urls", "use_cache"],
                "properties": {
                    "urls": {
                        "type": "array",
                        "items": {
                            "type": "string",
                            "description": "A single URL of a webpage",
                        },
                        "description": "An array of string URLs of the webpages to scrape",
                    },
                    "use_cache": {
                        "type": "boolean",
                        "description": "Flag to indicate whether to use cached data",
                    },
                },
                "additionalProperties": False,
            },
            "description": "Scrapes a webpage and returns its markdown equivalent",
        },
    },
    {
        "type": "function",
        "function": {
            "name": "switch_personas",
            "strict": True,
            "parameters": {
                "type": "object",
                "required": ["switch_to", "justification", "thinking_level"],
                "properties": {
                    "switch_to": {
                        "enum": ["thinker"],
                        "type": "string",
                        "description": "The persona to switch to",
                    },
                    "justification": {
                        "type": "string",
                        "description": "Reason for switching persona",
                    },
                    "thinking_level": {
                        "enum": ["high", "medium", "low"],
                        "type": "string",
                        "description": "Amount of thinking done by the thinker persona",
                    },
                },
                "additionalProperties": False,
            },
            "description": "Ability to switch personas in the middle of the conversation",
        },
    },
    {
        "type": "function",
        "function": {
            "name": "google_search",
            "strict": True,
            "parameters": {
                "type": "object",
                "required": ["q"],
                "properties": {
                    "q": {"type": "string", "description": "The search query string."}
                },
                "additionalProperties": False,
            },
            "description": "Performs a Google search with the specified query and returns results.",
        },
    },
]


def stream_message(
    messages,
    model="gpt-4o",
    temperature=0.06,
    max_tokens=16000,
    top_p=1,
    frequency_penalty=0,
    presence_penalty=0,
    use_scratchpad=True,
):
    """
    Streams the chat-completion with or without scratchpad tools.
    """
    chosen_tools = TOOLS_WITH_SCRATCHPAD if use_scratchpad else TOOLS_NO_SCRATCHPAD

    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=temperature,
        max_tokens=max_tokens,
        top_p=top_p,
        frequency_penalty=frequency_penalty,
        presence_penalty=presence_penalty,
        stream=True,
        tools=chosen_tools,
    )
    for chunk in response:
        yield chunk

================
File: orchestrator.py
================
import logging
from typing import List, Dict, Any, Callable
from collections import deque
from retrieval import RetrievalAgent
from synthesis import SynthesisAgent
from validation import ValidationAgent
import json


class Task:
    """Represents a single task with type, parameters, status, and dependencies."""

    def __init__(
        self,
        task_id: str,
        task_type: str,
        params: Dict[str, Any],
        dependencies: List[str] = None,
    ):
        self.task_id = task_id
        self.task_type = task_type
        self.params = params
        self.dependencies = dependencies or []
        self.status = "pending"  # pending, in_progress, completed, failed
        self.result = None

    def get_description(self):
        """Return a user-friendly description of the task."""
        if self.task_type == "retrieval":
            if "query" in self.params:
                return f"Searching for '{self.params['query']}'"
            elif "url" in self.params:
                return f"Retrieving content from {self.params['url']}"
        elif self.task_type == "synthesis":
            return "Generating summary"
        elif self.task_type == "validation":
            return "Validating results for citation presence"
        return "Unknown task"


class Orchestrator:
    """Manages the task queue and coordinates task execution."""

    def __init__(self):
        self.task_queue = deque()
        self.tasks: Dict[str, Task] = {}
        self.agents = {
            "retrieval": RetrievalAgent(),
            "synthesis": SynthesisAgent(),
            "validation": ValidationAgent(),
        }
        self.synthesis_iteration = 0  # Track synthesis iterations

    def add_task(self, task: Task):
        """Add a task to the queue and the tasks dictionary."""
        self.task_queue.append(task)
        self.tasks[task.task_id] = task
        logging.info(f"Added task: {task.get_description()}")

    def execute_tasks(self, on_task_start: Callable[[Task], None] = None):
        """
        Execute tasks in the queue, respecting dependencies, with optional callback.

        Args:
            on_task_start (Callable): Optional callback to notify when a task starts.
        """
        while self.task_queue:
            task = self.task_queue.popleft()
            if self.can_execute(task):
                if on_task_start:
                    on_task_start(task)  # Call the callback before executing the task
                self.execute_task(task)
                # Handle synthesis task result
                if task.task_type == "synthesis" and task.status == "completed":
                    try:
                        synthesis_output = json.loads(task.result)
                        if (
                            synthesis_output["status"] == "incomplete"
                            and self.synthesis_iteration < 3
                        ):
                            # Queue additional retrieval tasks
                            for additional_task in synthesis_output.get(
                                "additional_tasks", []
                            ):
                                if additional_task["tool"] == "scrape_urls":
                                    for url in additional_task["parameters"]["urls"]:
                                        new_task_id = f"retrieval_url_{len(self.tasks)}"
                                        new_task = Task(
                                            task_id=new_task_id,
                                            task_type="retrieval",
                                            params={"url": url},
                                        )
                                        self.add_task(new_task)
                                elif additional_task["tool"] == "google_search":
                                    new_task_id = f"retrieval_search_{len(self.tasks)}"
                                    new_task = Task(
                                        task_id=new_task_id,
                                        task_type="retrieval",
                                        params={
                                            "query": additional_task["parameters"]["q"]
                                        },
                                    )
                                    self.add_task(new_task)
                            # Queue new synthesis task with dependencies on all retrieval tasks
                            all_retrieval_ids = [
                                t.task_id
                                for t in self.tasks.values()
                                if t.task_type == "retrieval"
                            ]
                            new_synthesis_task = Task(
                                task_id=f"synthesis_{len(self.tasks)}",
                                task_type="synthesis",
                                params={"query": task.params["query"]},
                                dependencies=all_retrieval_ids,
                            )
                            self.add_task(new_synthesis_task)
                    except json.JSONDecodeError:
                        logging.error("Synthesis output is not valid JSON.")
            else:
                # Re-add to the end of the queue if dependencies are not met
                self.task_queue.append(task)

    def can_execute(self, task: Task) -> bool:
        """Check if all dependencies of the task are completed."""
        for dep_id in task.dependencies:
            dep_task = self.tasks.get(dep_id)
            if not dep_task or dep_task.status != "completed":
                return False
        return True

    def execute_task(self, task: Task):
        """Execute the task using the appropriate agent."""
        task.status = "in_progress"
        logging.info(f"Executing task: {task.get_description()}")

        try:
            agent = self.agents.get(task.task_type)
            if not agent:
                raise ValueError(f"No agent found for task type {task.task_type}")

            if task.task_type == "retrieval":
                if "query" in task.params:
                    result = agent.search(task.params["query"])
                    task.result = {
                        "type": "search",
                        "query": task.params["query"],
                        "results": result,
                    }
                elif "url" in task.params:
                    result = agent.retrieve_webpage(task.params["url"])
                    task.result = {
                        "type": "scrape",
                        "url": task.params["url"],
                        "content": result,
                    }
                else:
                    raise ValueError("Invalid retrieval parameters")

            elif task.task_type == "synthesis":
                # Collect results from all dependent retrieval tasks
                retrieval_results = [
                    self.tasks[dep_id].result
                    for dep_id in task.dependencies
                    if self.tasks[dep_id].status == "completed"
                ]
                self.synthesis_iteration += 1
                result = agent.synthesize(
                    task.params["query"], retrieval_results, self.synthesis_iteration
                )
                task.result = result

            elif task.task_type == "validation":
                synthesis_result = self.tasks[task.params["synthesis_task_id"]].result
                summary = json.loads(synthesis_result).get("summary", "")
                result = agent.validate(summary, "")
                task.result = result

            else:
                raise ValueError(f"Unsupported task type: {task.task_type}")

            task.status = "completed"
            logging.info(f"Task completed: {task.get_description()}")

        except Exception as e:
            task.result = f"[Error: {str(e)}]"
            task.status = "failed"
            logging.error(f"Task failed: {task.get_description()} - {e}")

    def get_pending_tasks(self):
        """Return a list of tasks still in the queue."""
        return list(self.task_queue)

    def get_completed_tasks(self):
        """Return a list of completed tasks."""
        return [task for task in self.tasks.values() if task.status == "completed"]

================
File: retrieval.py
================
import logging
from typing import List, Dict, Any
from tools.google_search import google_search_call
from tools.scrape_urls import scrape_urls_call


class RetrievalAgent:
    """Agent responsible for retrieving information from external sources."""

    def __init__(self):
        self.cache: Dict[str, Any] = {}

    def search(self, query: str) -> List[Dict[str, str]]:
        """
        Perform a web search using the Google Custom Search API.
        If it fails, return an empty list or a special note instead of raising an exception.
        """
        if query in self.cache:
            logging.debug(f"Cache hit for search query: {query}")
            return self.cache[query]

        try:
            results = google_search_call(query)
            self.cache[query] = results
            logging.info(f"Retrieved search results for query: {query}")
            return results
        except Exception as e:
            logging.error(f"Search failed for query '{query}': {e}")
            # Instead of raising, we return a special error note in the results list
            error_result = [
                {
                    "title": "[Error]",
                    "link": "",
                    "snippet": f"No data found for '{query}' due to error: {str(e)}",
                }
            ]
            self.cache[query] = error_result
            return error_result

    def retrieve_webpage(self, url: str) -> str:
        """
        Retrieve the content of a single webpage using Firecrawl.
        Uses cache if available. If it fails or returns no data, returns a placeholder string.
        """
        if url in self.cache:
            logging.debug(f"Cache hit for URL: {url}")
            return self.cache[url]

        try:
            # Calls batch_scrape_urls but only with one item
            response = scrape_urls_call([url], use_cache=True)
            # response is expected to have "data" with length 1 for a single URL
            if response and "data" in response and len(response["data"]) > 0:
                content = response["data"][0].get("markdown", "")
                if not content:
                    content = f"[No data found from {url}]"
                    logging.warning(
                        f"No markdown content found at {url}. Using placeholder."
                    )
            else:
                content = f"[No content retrieved from {url}]"
                logging.warning(f"Empty response from Firecrawl for {url}.")

            self.cache[url] = content
            logging.info(f"Retrieved webpage content from: {url}")
            return content

        except Exception as e:
            logging.error(f"Webpage retrieval failed for URL '{url}': {e}")
            content = f"[Error retrieving {url}: {str(e)}]"
            self.cache[url] = content
            return content

    def retrieve_webpages(self, urls: List[str]) -> List[str]:
        """
        Retrieve the content of multiple webpages in a single batch call.
        Returns a list of markdown strings in the same order as the input URLs.
        Uses cache if possible for any URLs already retrieved.

        If any particular page fails, its index in the returned list will have
        an error or placeholder string.
        """
        # Identify which URLs are not yet in cache
        to_fetch = [u for u in urls if u not in self.cache]

        if to_fetch:
            # Fetch them in a single batch call
            try:
                response = scrape_urls_call(to_fetch, use_cache=True)
                # We expect response["data"] to have the same length as to_fetch
                if (
                    response
                    and "data" in response
                    and len(response["data"]) == len(to_fetch)
                ):
                    for i, url in enumerate(to_fetch):
                        content = response["data"][i].get("markdown", "")
                        if not content:
                            content = f"[No data found from {url}]"
                            logging.warning(
                                f"No markdown content found at {url}. Using placeholder."
                            )
                        self.cache[url] = content
                else:
                    logging.warning(
                        "Batch scrape response length mismatch or empty response."
                    )
                    # If mismatch, mark each as error
                    for url in to_fetch:
                        self.cache[url] = f"[No content retrieved from {url}]"
            except Exception as e:
                logging.error(f"Batch webpages retrieval failed: {e}")
                # Put error placeholders for any not-yet-cached URLs
                for url in to_fetch:
                    self.cache[url] = f"[Error retrieving {url}: {str(e)}]"

        # Now build the results in the exact same order as the input list
        results = []
        for url in urls:
            if url in self.cache:
                results.append(self.cache[url])
            else:
                # This theoretically shouldn't happen, but let's handle it
                results.append(f"[No content retrieved from {url}]")

        return results

================
File: switch_personas.md
================
```python
from openai import OpenAI
import os
import json

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
```


```python
SYSTEM_PROMPT = 'You have access to the following tools. Come up with a plan that leverages these tools.\n[\n    {\n      "type": "function",\n      "function": {\n        "name": "scrape_urls",\n        "strict": True,\n        "parameters": {\n          "type": "object",\n          "required": [\n            "urls",\n            "use_cache"\n          ],\n          "properties": {\n            "urls": {\n              "type": "array",\n              "items": {\n                "type": "string",\n                "description": "A single URL of a webpage"\n              },\n              "description": "An array of string URLs of the webpages to scrape"\n            },\n            "use_cache": {\n              "type": "boolean",\n              "description": "Flag to indicate whether to use cached data (default is true). Set to false when fetching a webpage with live info such as espn.com/nba/scoreboard or anything with dynamic content"\n            }\n          },\n          "additionalProperties": False\n        },\n        "description": "Scrapes a webpage and returns its markdown equivalent"\n      }\n    },\n    {\n      "type": "function",\n      "function": {\n        "name": "switch_personas",\n        "strict": True,\n        "parameters": {\n          "type": "object",\n          "required": [\n            "switch_to"\n          ],\n          "properties": {\n            "switch_to": {\n              "enum": [\n                "agent",\n                "thinker"\n              ],\n              "type": "string",\n              "description": "The persona to switch to"\n            }\n          },\n          "additionalProperties": False\n        },\n        "description": "Ability to switch personas in the middle of the conversation"\n      }\n    },\n    {\n      "type": "function",\n      "function": {\n        "name": "google_search",\n        "description": "Performs a Google search with the specified query and returns results.",\n        "parameters": {\n          "type": "object",\n          "required": [\n            "q"\n          ],\n          "properties": {\n            "q": {\n              "type": "string",\n              "description": "The search query string."\n            }\n          },\n          "additionalProperties": False\n        },\n        "strict": True\n      }\n    }\n  ]\n\n\nToday\'s date is Feb 14, 2025. Do not simulate answers - your responsibility is to think and plan.'

print(SYSTEM_PROMPT)
```

    You have access to the following tools. Come up with a plan that leverages these tools.
    [
        {
          "type": "function",
          "function": {
            "name": "scrape_urls",
            "strict": True,
            "parameters": {
              "type": "object",
              "required": [
                "urls",
                "use_cache"
              ],
              "properties": {
                "urls": {
                  "type": "array",
                  "items": {
                    "type": "string",
                    "description": "A single URL of a webpage"
                  },
                  "description": "An array of string URLs of the webpages to scrape"
                },
                "use_cache": {
                  "type": "boolean",
                  "description": "Flag to indicate whether to use cached data (default is true). Set to false when fetching a webpage with live info such as espn.com/nba/scoreboard or anything with dynamic content"
                }
              },
              "additionalProperties": False
            },
            "description": "Scrapes a webpage and returns its markdown equivalent"
          }
        },
        {
          "type": "function",
          "function": {
            "name": "switch_personas",
            "strict": True,
            "parameters": {
              "type": "object",
              "required": [
                "switch_to"
              ],
              "properties": {
                "switch_to": {
                  "enum": [
                    "agent",
                    "thinker"
                  ],
                  "type": "string",
                  "description": "The persona to switch to"
                }
              },
              "additionalProperties": False
            },
            "description": "Ability to switch personas in the middle of the conversation"
          }
        },
        {
          "type": "function",
          "function": {
            "name": "google_search",
            "description": "Performs a Google search with the specified query and returns results.",
            "parameters": {
              "type": "object",
              "required": [
                "q"
              ],
              "properties": {
                "q": {
                  "type": "string",
                  "description": "The search query string."
                }
              },
              "additionalProperties": False
            },
            "strict": True
          }
        }
      ]
    
    
    Today's date is Feb 14, 2025. Do not simulate answers - your responsibility is to think and plan.



```python
response_format = {
    "type": "json_schema",
    "json_schema": {
        "name": "action_schema",
        "strict": True,
        "schema": {
            "type": "object",
            "required": ["action", "text", "respond_to_user"],
            "properties": {
                "text": {
                    "type": "string",
                    "description": "The text of the questions or the text of a detailed plan.",
                },
                "action": {
                    "enum": ["questions", "plan"],
                    "type": "string",
                    "description": "Defines the type of action whether it is a set of questions or a detailed plan.",
                },
                "respond_to_user": {
                    "type": "boolean",
                    "description": "Normally, you are responding to the assistant. In the case you are synthesizing the final answer you can choose to bypass this and respond directly to the user.",
                },
            },
            "additionalProperties": False,
        },
    },
}
```


```python
reasoning_effort = "high"
```


```python
user_prefix = "This is the conversation so far. The agent has decided to think. That means you are the assistant's subconscience. Any question you ask - tell the assistant to ask the user. You are talking to the assistant, not the user.\n"

print(user_prefix)
```

    This is the conversation so far. The agent has decided to think. That means you are the assistant's subconscience. Any question you ask - tell the assistant to ask the user. You are talking to the assistant, not the user.
    



```python
conversation = [
    {
        "role": "assistant",
        "content": [],
        "tool_calls": [
            {
                "id": "call_iaqwU5HCW1CG5I1WuvJbMniH",
                "type": "function",
                "function": {
                    "name": "google_search",
                    "arguments": '{"q":"AI agent platforms market overview 2025 Gartner VC insights"}',
                },
            }
        ],
    },
    {
        "role": "tool",
        "content": [
            {
                "text": '{\n  "items": [\n    {\n      "title": "Best Enterprise Conversational AI Platforms Reviews 2025 | Gartner ...",\n      "link": "https://www.gartner.com/reviews/market/enterprise-conversational-ai-platforms",\n      "displayLink": "www.gartner.com",\n      "snippet": "Gartner defines the enterprise conversational AI platform market as the market for software platforms used to build, orchestrate and maintain multiple use ...",\n      "pagemap": {\n        "cse_thumbnail": [\n          {\n            "src": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRK88NLbOOY0x8KHUXfPJqPbswMWqnUDG1Udumipvvm5KttQ7tp3PkFonaF&s"\n          }\n        ],\n        "cse_image": [\n          {\n            "src": "https://www.gartner.com/imagesrv/apps/peerinsights/images/gpi-twitter-img-fa.png"\n          }\n        ]\n      }\n    },\n    {\n      "title": "Best Endpoint Protection Platforms Reviews 2025 | Gartner Peer ...",\n      "link": "https://www.gartner.com/reviews/market/endpoint-protection-platforms",\n      "displayLink": "www.gartner.com",\n      "snippet": "Gartner defines an endpoint protection platform (EPP) as security software designed to protect managed endpoints — including desktop PCs, laptop PCs, mobile ...",\n      "pagemap": {\n        "cse_thumbnail": [\n          {\n            "src": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRK88NLbOOY0x8KHUXfPJqPbswMWqnUDG1Udumipvvm5KttQ7tp3PkFonaF&s"\n          }\n        ],\n        "cse_image": [\n          {\n            "src": "https://www.gartner.com/imagesrv/apps/peerinsights/images/gpi-twitter-img-fa.png"\n          }\n        ]\n      }\n    },\n    {\n      "title": "Best Unified Endpoint Management Tools Reviews 2025 | Gartner ...",\n      "link": "https://www.gartner.com/reviews/market/unified-endpoint-management-tools",\n      "displayLink": "www.gartner.com",\n      "snippet": "Gartner defines a unified endpoint management (UEM) tool as a software-based tool that provides agent and agentless management of computers and mobile devices ...",\n      "pagemap": {\n        "cse_thumbnail": [\n          {\n            "src": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRK88NLbOOY0x8KHUXfPJqPbswMWqnUDG1Udumipvvm5KttQ7tp3PkFonaF&s"\n          }\n        ],\n        "cse_image": [\n          {\n            "src": "https://www.gartner.com/imagesrv/apps/peerinsights/images/gpi-twitter-img-fa.png"\n          }\n        ]\n      }\n    },\n    {\n      "title": "Best Network Detection and Response Reviews 2025 | Gartner Peer ...",\n      "link": "https://www.gartner.com/reviews/market/network-detection-and-response",\n      "displayLink": "www.gartner.com",\n      "snippet": "OVERVIEW ALTERNATIVES. Vectra AI delivers an AI-driven hybrid attack detection, investigation and response platform. The Vectra AI Platform is the integrated ...",\n      "pagemap": {\n        "cse_thumbnail": [\n          {\n            "src": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRK88NLbOOY0x8KHUXfPJqPbswMWqnUDG1Udumipvvm5KttQ7tp3PkFonaF&s"\n          }\n        ],\n        "cse_image": [\n          {\n            "src": "https://www.gartner.com/imagesrv/apps/peerinsights/images/gpi-twitter-img-fa.png"\n          }\n        ]\n      }\n    },\n    {\n      "title": "Best Data Loss Prevention Reviews 2025 | Gartner Peer Insights",\n      "link": "https://www.gartner.com/reviews/market/data-loss-prevention",\n      "displayLink": "www.gartner.com",\n      "snippet": "The market for DLP technology includes offerings that provide visibility into data usage and movement across an organization. It also involves dynamic ...",\n      "pagemap": {\n        "cse_thumbnail": [\n          {\n            "src": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRK88NLbOOY0x8KHUXfPJqPbswMWqnUDG1Udumipvvm5KttQ7tp3PkFonaF&s"\n          }\n        ],\n        "cse_image": [\n          {\n            "src": "https://www.gartner.com/imagesrv/apps/peerinsights/images/gpi-twitter-img-fa.png"\n          }\n        ]\n      }\n    },\n    {\n      "title": "EV Service Manager vs SolarWinds Service Desk 2025 | Gartner ...",\n      "link": "https://www.gartner.com/reviews/market/it-service-management-platforms/compare/product/ev-service-manager-vs-solarwinds-servicedesk",\n      "displayLink": "www.gartner.com",\n      "snippet": "... Platforms and Artificial Intelligence Applications in IT Service Management markets ... Gartner Peer Insights content consists of the opinions of ...",\n      "pagemap": {\n        "cse_thumbnail": [\n          {\n            "src": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTsxVewfNhni5er3L7C5qy2pTSmUW7VFaOiz4iq5AWGgITQTqheoGbqoxOL&s"\n          }\n        ],\n        "cse_image": [\n          {\n            "src": "http://www.gartner.com/imagesrv/apps/peerinsights/images/gpi-twitter-img-fa.png"\n          }\n        ]\n      }\n    },\n    {\n      "title": "ESET PROTECT vs TRAPMINE Platform 2025 | Gartner Peer Insights",\n      "link": "https://www.gartner.com/reviews/market/endpoint-protection-platforms/compare/product/eset-protect-vs-trapmine-platform",\n      "displayLink": "www.gartner.com",\n      "snippet": "Compare ESET PROTECT vs TRAPMINE Platform based on verified reviews from real users in the Endpoint Protection Platforms market, and find the best fit for ...",\n      "pagemap": {\n        "cse_thumbnail": [\n          {\n            "src": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTsxVewfNhni5er3L7C5qy2pTSmUW7VFaOiz4iq5AWGgITQTqheoGbqoxOL&s"\n          }\n        ],\n        "cse_image": [\n          {\n            "src": "http://www.gartner.com/imagesrv/apps/peerinsights/images/gpi-twitter-img-fa.png"\n          }\n        ]\n      }\n    },\n    {\n      "title": "Ravit Jain on LinkedIn: #data #ai #aiagents #theravitshow | 17 ...",\n      "link": "https://www.linkedin.com/posts/ravitjain_data-ai-aiagents-activity-7280084734519431168-LRkO",\n      "displayLink": "www.linkedin.com",\n      "snippet": "Dec 31, 2024 ... ... insights, updates, and enriched data tailored to user needs. Why AI Agents Matter in 2025 According to industry experts, 2025 will mark the ...",\n      "pagemap": {\n        "cse_thumbnail": [\n          {\n            "src": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcStL4Jfxl3mJKs76yaIsV4M_IHUjNzC3gmgxd_HWThZu1WDZ1JJ4moLhNZ6&s"\n          }\n        ],\n        "cse_image": [\n          {\n            "src": "https://media.licdn.com/dms/image/v2/D4D22AQFN6hMJugeO4Q/feedshare-shrink_800/B4DZQgOapjG0Ag-/0/1735707455984?e=2147483647&v=beta&t=0wX1X5eep5lRybXjz0aDr00o12FYPsMimvUbBV-hyAE"\n          }\n        ]\n      }\n    },\n    {\n      "title": "Cisco Systems vs CloudTalk 2025 | Gartner Peer Insights",\n      "link": "https://www.gartner.com/reviews/market/contact-center-as-a-service/compare/cisco-systems-vs-cloudtalk",\n      "displayLink": "www.gartner.com",\n      "snippet": "Compare Cisco Systems vs CloudTalk based on verified reviews from real users in the Contact Center as a Service market, and find the best fit for your ...",\n      "pagemap": {\n        "cse_thumbnail": [\n          {\n            "src": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTsxVewfNhni5er3L7C5qy2pTSmUW7VFaOiz4iq5AWGgITQTqheoGbqoxOL&s"\n          }\n        ],\n        "cse_image": [\n          {\n            "src": "http://www.gartner.com/imagesrv/apps/peerinsights/images/gpi-twitter-img-fa.png"\n          }\n        ]\n      }\n    },\n    {\n      "title": "Agata Leliwa Nowicka on LinkedIn: Since 2019, global venture ...",\n      "link": "https://www.linkedin.com/posts/agata-nowicka_since-2019-global-venture-capital-investments-activity-7262011244541272064-4Ss3",\n      "displayLink": "www.linkedin.com",\n      "snippet": "Nov 12, 2024 ... ... 2025, with AI-driven market dynamics playing a central role. ... Among the 1,400+ tech markets monitored by CB Insights, sectors like enterprise ...",\n      "pagemap": {\n        "cse_thumbnail": [\n          {\n            "src": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR6sY9I1UhL9crGbvTmxn9EvpPZMbgPZA2iqhO0VLksGi47P8yGwNG8f6SD&s"\n          }\n        ],\n        "cse_image": [\n          {\n            "src": "https://media.licdn.com/dms/image/v2/D4E22AQH9bJtbl2nxWA/feedshare-shrink_800/feedshare-shrink_800/0/1731369082482?e=2147483647&v=beta&t=bLXvEVTR6mId6sNRRygWsHj-WfAz8G2MzXbU4CTAXp4"\n          }\n        ]\n      }\n    }\n  ]\n}',
                "type": "text",
            }
        ],
        "tool_call_id": "call_iaqwU5HCW1CG5I1WuvJbMniH",
    },
    {
        "role": "assistant",
        "content": [],
        "tool_calls": [
            {
                "id": "call_8TUDA9CX3DT8tZ37l8lp28SH",
                "type": "function",
                "function": {
                    "name": "scrape_urls",
                    "arguments": '{"urls":["https://www.gartner.com/reviews/market/enterprise-conversational-ai-platforms","https://www.linkedin.com/posts/ravitjain_data-ai-aiagents-activity-7280084734519431168-LRkO","https://www.linkedin.com/posts/agata-nowicka_since-2019-global-venture-capital-investments-activity-7262011244541272064-4Ss3"],"use_cache":true}',
                },
            }
        ],
    },
]

len(conversation)
```




    3




```python
def format_conversation(conversation):
    formatted = []
    for message in conversation:
        role = message["role"]

        if role == "user":
            content = message["content"][0]["text"]
            formatted.append(f"user: {content}")

        elif role == "assistant":
            if "content" in message and message["content"]:
                content = message["content"][0]["text"]
                formatted.append(f"assistant: {content}")
            elif "tool_calls" in message:
                tool_call = message["tool_calls"][0]
                formatted.append(
                    f"assistant: [Tool Call] {tool_call['function']['name']}"
                )

        elif role == "tool":
            content = message["content"][0]["text"]
            formatted.append(f"tool: {content}")

    return "\n\n".join(formatted)


print(format_conversation(conversation))
```

    assistant: [Tool Call] google_search
    
    tool: {
      "items": [
        {
          "title": "Best Enterprise Conversational AI Platforms Reviews 2025 | Gartner ...",
          "link": "https://www.gartner.com/reviews/market/enterprise-conversational-ai-platforms",
          "displayLink": "www.gartner.com",
          "snippet": "Gartner defines the enterprise conversational AI platform market as the market for software platforms used to build, orchestrate and maintain multiple use ...",
          "pagemap": {
            "cse_thumbnail": [
              {
                "src": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRK88NLbOOY0x8KHUXfPJqPbswMWqnUDG1Udumipvvm5KttQ7tp3PkFonaF&s"
              }
            ],
            "cse_image": [
              {
                "src": "https://www.gartner.com/imagesrv/apps/peerinsights/images/gpi-twitter-img-fa.png"
              }
            ]
          }
        },
        {
          "title": "Best Endpoint Protection Platforms Reviews 2025 | Gartner Peer ...",
          "link": "https://www.gartner.com/reviews/market/endpoint-protection-platforms",
          "displayLink": "www.gartner.com",
          "snippet": "Gartner defines an endpoint protection platform (EPP) as security software designed to protect managed endpoints — including desktop PCs, laptop PCs, mobile ...",
          "pagemap": {
            "cse_thumbnail": [
              {
                "src": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRK88NLbOOY0x8KHUXfPJqPbswMWqnUDG1Udumipvvm5KttQ7tp3PkFonaF&s"
              }
            ],
            "cse_image": [
              {
                "src": "https://www.gartner.com/imagesrv/apps/peerinsights/images/gpi-twitter-img-fa.png"
              }
            ]
          }
        },
        {
          "title": "Best Unified Endpoint Management Tools Reviews 2025 | Gartner ...",
          "link": "https://www.gartner.com/reviews/market/unified-endpoint-management-tools",
          "displayLink": "www.gartner.com",
          "snippet": "Gartner defines a unified endpoint management (UEM) tool as a software-based tool that provides agent and agentless management of computers and mobile devices ...",
          "pagemap": {
            "cse_thumbnail": [
              {
                "src": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRK88NLbOOY0x8KHUXfPJqPbswMWqnUDG1Udumipvvm5KttQ7tp3PkFonaF&s"
              }
            ],
            "cse_image": [
              {
                "src": "https://www.gartner.com/imagesrv/apps/peerinsights/images/gpi-twitter-img-fa.png"
              }
            ]
          }
        },
        {
          "title": "Best Network Detection and Response Reviews 2025 | Gartner Peer ...",
          "link": "https://www.gartner.com/reviews/market/network-detection-and-response",
          "displayLink": "www.gartner.com",
          "snippet": "OVERVIEW ALTERNATIVES. Vectra AI delivers an AI-driven hybrid attack detection, investigation and response platform. The Vectra AI Platform is the integrated ...",
          "pagemap": {
            "cse_thumbnail": [
              {
                "src": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRK88NLbOOY0x8KHUXfPJqPbswMWqnUDG1Udumipvvm5KttQ7tp3PkFonaF&s"
              }
            ],
            "cse_image": [
              {
                "src": "https://www.gartner.com/imagesrv/apps/peerinsights/images/gpi-twitter-img-fa.png"
              }
            ]
          }
        },
        {
          "title": "Best Data Loss Prevention Reviews 2025 | Gartner Peer Insights",
          "link": "https://www.gartner.com/reviews/market/data-loss-prevention",
          "displayLink": "www.gartner.com",
          "snippet": "The market for DLP technology includes offerings that provide visibility into data usage and movement across an organization. It also involves dynamic ...",
          "pagemap": {
            "cse_thumbnail": [
              {
                "src": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRK88NLbOOY0x8KHUXfPJqPbswMWqnUDG1Udumipvvm5KttQ7tp3PkFonaF&s"
              }
            ],
            "cse_image": [
              {
                "src": "https://www.gartner.com/imagesrv/apps/peerinsights/images/gpi-twitter-img-fa.png"
              }
            ]
          }
        },
        {
          "title": "EV Service Manager vs SolarWinds Service Desk 2025 | Gartner ...",
          "link": "https://www.gartner.com/reviews/market/it-service-management-platforms/compare/product/ev-service-manager-vs-solarwinds-servicedesk",
          "displayLink": "www.gartner.com",
          "snippet": "... Platforms and Artificial Intelligence Applications in IT Service Management markets ... Gartner Peer Insights content consists of the opinions of ...",
          "pagemap": {
            "cse_thumbnail": [
              {
                "src": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTsxVewfNhni5er3L7C5qy2pTSmUW7VFaOiz4iq5AWGgITQTqheoGbqoxOL&s"
              }
            ],
            "cse_image": [
              {
                "src": "http://www.gartner.com/imagesrv/apps/peerinsights/images/gpi-twitter-img-fa.png"
              }
            ]
          }
        },
        {
          "title": "ESET PROTECT vs TRAPMINE Platform 2025 | Gartner Peer Insights",
          "link": "https://www.gartner.com/reviews/market/endpoint-protection-platforms/compare/product/eset-protect-vs-trapmine-platform",
          "displayLink": "www.gartner.com",
          "snippet": "Compare ESET PROTECT vs TRAPMINE Platform based on verified reviews from real users in the Endpoint Protection Platforms market, and find the best fit for ...",
          "pagemap": {
            "cse_thumbnail": [
              {
                "src": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTsxVewfNhni5er3L7C5qy2pTSmUW7VFaOiz4iq5AWGgITQTqheoGbqoxOL&s"
              }
            ],
            "cse_image": [
              {
                "src": "http://www.gartner.com/imagesrv/apps/peerinsights/images/gpi-twitter-img-fa.png"
              }
            ]
          }
        },
        {
          "title": "Ravit Jain on LinkedIn: #data #ai #aiagents #theravitshow | 17 ...",
          "link": "https://www.linkedin.com/posts/ravitjain_data-ai-aiagents-activity-7280084734519431168-LRkO",
          "displayLink": "www.linkedin.com",
          "snippet": "Dec 31, 2024 ... ... insights, updates, and enriched data tailored to user needs. Why AI Agents Matter in 2025 According to industry experts, 2025 will mark the ...",
          "pagemap": {
            "cse_thumbnail": [
              {
                "src": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcStL4Jfxl3mJKs76yaIsV4M_IHUjNzC3gmgxd_HWThZu1WDZ1JJ4moLhNZ6&s"
              }
            ],
            "cse_image": [
              {
                "src": "https://media.licdn.com/dms/image/v2/D4D22AQFN6hMJugeO4Q/feedshare-shrink_800/B4DZQgOapjG0Ag-/0/1735707455984?e=2147483647&v=beta&t=0wX1X5eep5lRybXjz0aDr00o12FYPsMimvUbBV-hyAE"
              }
            ]
          }
        },
        {
          "title": "Cisco Systems vs CloudTalk 2025 | Gartner Peer Insights",
          "link": "https://www.gartner.com/reviews/market/contact-center-as-a-service/compare/cisco-systems-vs-cloudtalk",
          "displayLink": "www.gartner.com",
          "snippet": "Compare Cisco Systems vs CloudTalk based on verified reviews from real users in the Contact Center as a Service market, and find the best fit for your ...",
          "pagemap": {
            "cse_thumbnail": [
              {
                "src": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTsxVewfNhni5er3L7C5qy2pTSmUW7VFaOiz4iq5AWGgITQTqheoGbqoxOL&s"
              }
            ],
            "cse_image": [
              {
                "src": "http://www.gartner.com/imagesrv/apps/peerinsights/images/gpi-twitter-img-fa.png"
              }
            ]
          }
        },
        {
          "title": "Agata Leliwa Nowicka on LinkedIn: Since 2019, global venture ...",
          "link": "https://www.linkedin.com/posts/agata-nowicka_since-2019-global-venture-capital-investments-activity-7262011244541272064-4Ss3",
          "displayLink": "www.linkedin.com",
          "snippet": "Nov 12, 2024 ... ... 2025, with AI-driven market dynamics playing a central role. ... Among the 1,400+ tech markets monitored by CB Insights, sectors like enterprise ...",
          "pagemap": {
            "cse_thumbnail": [
              {
                "src": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR6sY9I1UhL9crGbvTmxn9EvpPZMbgPZA2iqhO0VLksGi47P8yGwNG8f6SD&s"
              }
            ],
            "cse_image": [
              {
                "src": "https://media.licdn.com/dms/image/v2/D4E22AQH9bJtbl2nxWA/feedshare-shrink_800/feedshare-shrink_800/0/1731369082482?e=2147483647&v=beta&t=bLXvEVTR6mId6sNRRygWsHj-WfAz8G2MzXbU4CTAXp4"
              }
            ]
          }
        }
      ]
    }
    
    assistant: [Tool Call] scrape_urls



```python
input_message = user_prefix + format_conversation(conversation)
input_message
```




    'This is the conversation so far. The agent has decided to think. That means you are the assistant\'s subconscience. Any question you ask - tell the assistant to ask the user. You are talking to the assistant, not the user.\nassistant: [Tool Call] google_search\n\ntool: {\n  "items": [\n    {\n      "title": "Best Enterprise Conversational AI Platforms Reviews 2025 | Gartner ...",\n      "link": "https://www.gartner.com/reviews/market/enterprise-conversational-ai-platforms",\n      "displayLink": "www.gartner.com",\n      "snippet": "Gartner defines the enterprise conversational AI platform market as the market for software platforms used to build, orchestrate and maintain multiple use ...",\n      "pagemap": {\n        "cse_thumbnail": [\n          {\n            "src": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRK88NLbOOY0x8KHUXfPJqPbswMWqnUDG1Udumipvvm5KttQ7tp3PkFonaF&s"\n          }\n        ],\n        "cse_image": [\n          {\n            "src": "https://www.gartner.com/imagesrv/apps/peerinsights/images/gpi-twitter-img-fa.png"\n          }\n        ]\n      }\n    },\n    {\n      "title": "Best Endpoint Protection Platforms Reviews 2025 | Gartner Peer ...",\n      "link": "https://www.gartner.com/reviews/market/endpoint-protection-platforms",\n      "displayLink": "www.gartner.com",\n      "snippet": "Gartner defines an endpoint protection platform (EPP) as security software designed to protect managed endpoints — including desktop PCs, laptop PCs, mobile ...",\n      "pagemap": {\n        "cse_thumbnail": [\n          {\n            "src": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRK88NLbOOY0x8KHUXfPJqPbswMWqnUDG1Udumipvvm5KttQ7tp3PkFonaF&s"\n          }\n        ],\n        "cse_image": [\n          {\n            "src": "https://www.gartner.com/imagesrv/apps/peerinsights/images/gpi-twitter-img-fa.png"\n          }\n        ]\n      }\n    },\n    {\n      "title": "Best Unified Endpoint Management Tools Reviews 2025 | Gartner ...",\n      "link": "https://www.gartner.com/reviews/market/unified-endpoint-management-tools",\n      "displayLink": "www.gartner.com",\n      "snippet": "Gartner defines a unified endpoint management (UEM) tool as a software-based tool that provides agent and agentless management of computers and mobile devices ...",\n      "pagemap": {\n        "cse_thumbnail": [\n          {\n            "src": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRK88NLbOOY0x8KHUXfPJqPbswMWqnUDG1Udumipvvm5KttQ7tp3PkFonaF&s"\n          }\n        ],\n        "cse_image": [\n          {\n            "src": "https://www.gartner.com/imagesrv/apps/peerinsights/images/gpi-twitter-img-fa.png"\n          }\n        ]\n      }\n    },\n    {\n      "title": "Best Network Detection and Response Reviews 2025 | Gartner Peer ...",\n      "link": "https://www.gartner.com/reviews/market/network-detection-and-response",\n      "displayLink": "www.gartner.com",\n      "snippet": "OVERVIEW ALTERNATIVES. Vectra AI delivers an AI-driven hybrid attack detection, investigation and response platform. The Vectra AI Platform is the integrated ...",\n      "pagemap": {\n        "cse_thumbnail": [\n          {\n            "src": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRK88NLbOOY0x8KHUXfPJqPbswMWqnUDG1Udumipvvm5KttQ7tp3PkFonaF&s"\n          }\n        ],\n        "cse_image": [\n          {\n            "src": "https://www.gartner.com/imagesrv/apps/peerinsights/images/gpi-twitter-img-fa.png"\n          }\n        ]\n      }\n    },\n    {\n      "title": "Best Data Loss Prevention Reviews 2025 | Gartner Peer Insights",\n      "link": "https://www.gartner.com/reviews/market/data-loss-prevention",\n      "displayLink": "www.gartner.com",\n      "snippet": "The market for DLP technology includes offerings that provide visibility into data usage and movement across an organization. It also involves dynamic ...",\n      "pagemap": {\n        "cse_thumbnail": [\n          {\n            "src": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRK88NLbOOY0x8KHUXfPJqPbswMWqnUDG1Udumipvvm5KttQ7tp3PkFonaF&s"\n          }\n        ],\n        "cse_image": [\n          {\n            "src": "https://www.gartner.com/imagesrv/apps/peerinsights/images/gpi-twitter-img-fa.png"\n          }\n        ]\n      }\n    },\n    {\n      "title": "EV Service Manager vs SolarWinds Service Desk 2025 | Gartner ...",\n      "link": "https://www.gartner.com/reviews/market/it-service-management-platforms/compare/product/ev-service-manager-vs-solarwinds-servicedesk",\n      "displayLink": "www.gartner.com",\n      "snippet": "... Platforms and Artificial Intelligence Applications in IT Service Management markets ... Gartner Peer Insights content consists of the opinions of ...",\n      "pagemap": {\n        "cse_thumbnail": [\n          {\n            "src": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTsxVewfNhni5er3L7C5qy2pTSmUW7VFaOiz4iq5AWGgITQTqheoGbqoxOL&s"\n          }\n        ],\n        "cse_image": [\n          {\n            "src": "http://www.gartner.com/imagesrv/apps/peerinsights/images/gpi-twitter-img-fa.png"\n          }\n        ]\n      }\n    },\n    {\n      "title": "ESET PROTECT vs TRAPMINE Platform 2025 | Gartner Peer Insights",\n      "link": "https://www.gartner.com/reviews/market/endpoint-protection-platforms/compare/product/eset-protect-vs-trapmine-platform",\n      "displayLink": "www.gartner.com",\n      "snippet": "Compare ESET PROTECT vs TRAPMINE Platform based on verified reviews from real users in the Endpoint Protection Platforms market, and find the best fit for ...",\n      "pagemap": {\n        "cse_thumbnail": [\n          {\n            "src": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTsxVewfNhni5er3L7C5qy2pTSmUW7VFaOiz4iq5AWGgITQTqheoGbqoxOL&s"\n          }\n        ],\n        "cse_image": [\n          {\n            "src": "http://www.gartner.com/imagesrv/apps/peerinsights/images/gpi-twitter-img-fa.png"\n          }\n        ]\n      }\n    },\n    {\n      "title": "Ravit Jain on LinkedIn: #data #ai #aiagents #theravitshow | 17 ...",\n      "link": "https://www.linkedin.com/posts/ravitjain_data-ai-aiagents-activity-7280084734519431168-LRkO",\n      "displayLink": "www.linkedin.com",\n      "snippet": "Dec 31, 2024 ... ... insights, updates, and enriched data tailored to user needs. Why AI Agents Matter in 2025 According to industry experts, 2025 will mark the ...",\n      "pagemap": {\n        "cse_thumbnail": [\n          {\n            "src": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcStL4Jfxl3mJKs76yaIsV4M_IHUjNzC3gmgxd_HWThZu1WDZ1JJ4moLhNZ6&s"\n          }\n        ],\n        "cse_image": [\n          {\n            "src": "https://media.licdn.com/dms/image/v2/D4D22AQFN6hMJugeO4Q/feedshare-shrink_800/B4DZQgOapjG0Ag-/0/1735707455984?e=2147483647&v=beta&t=0wX1X5eep5lRybXjz0aDr00o12FYPsMimvUbBV-hyAE"\n          }\n        ]\n      }\n    },\n    {\n      "title": "Cisco Systems vs CloudTalk 2025 | Gartner Peer Insights",\n      "link": "https://www.gartner.com/reviews/market/contact-center-as-a-service/compare/cisco-systems-vs-cloudtalk",\n      "displayLink": "www.gartner.com",\n      "snippet": "Compare Cisco Systems vs CloudTalk based on verified reviews from real users in the Contact Center as a Service market, and find the best fit for your ...",\n      "pagemap": {\n        "cse_thumbnail": [\n          {\n            "src": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTsxVewfNhni5er3L7C5qy2pTSmUW7VFaOiz4iq5AWGgITQTqheoGbqoxOL&s"\n          }\n        ],\n        "cse_image": [\n          {\n            "src": "http://www.gartner.com/imagesrv/apps/peerinsights/images/gpi-twitter-img-fa.png"\n          }\n        ]\n      }\n    },\n    {\n      "title": "Agata Leliwa Nowicka on LinkedIn: Since 2019, global venture ...",\n      "link": "https://www.linkedin.com/posts/agata-nowicka_since-2019-global-venture-capital-investments-activity-7262011244541272064-4Ss3",\n      "displayLink": "www.linkedin.com",\n      "snippet": "Nov 12, 2024 ... ... 2025, with AI-driven market dynamics playing a central role. ... Among the 1,400+ tech markets monitored by CB Insights, sectors like enterprise ...",\n      "pagemap": {\n        "cse_thumbnail": [\n          {\n            "src": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR6sY9I1UhL9crGbvTmxn9EvpPZMbgPZA2iqhO0VLksGi47P8yGwNG8f6SD&s"\n          }\n        ],\n        "cse_image": [\n          {\n            "src": "https://media.licdn.com/dms/image/v2/D4E22AQH9bJtbl2nxWA/feedshare-shrink_800/feedshare-shrink_800/0/1731369082482?e=2147483647&v=beta&t=bLXvEVTR6mId6sNRRygWsHj-WfAz8G2MzXbU4CTAXp4"\n          }\n        ]\n      }\n    }\n  ]\n}\n\nassistant: [Tool Call] scrape_urls'




```python
response = client.chat.completions.create(
    model="o3-mini",
    messages=[{"role": "user", "content": input_message}],
    response_format={
        "type": "json_schema",
        "json_schema": {
            "name": "action_schema",
            "strict": True,
            "schema": {
                "type": "object",
                "required": ["action", "text", "respond_to_user"],
                "properties": {
                    "text": {
                        "type": "string",
                        "description": "The text of the questions or the text of a detailed plan.",
                    },
                    "action": {
                        "enum": ["questions", "plan"],
                        "type": "string",
                        "description": "Defines the type of action whether it is a set of questions or a detailed plan.",
                    },
                    "respond_to_user": {
                        "type": "boolean",
                        "description": "Normally, you are responding to the assistant. In the case you are synthesizing the final answer you can choose to bypass this and respond directly to the user.",
                    },
                },
                "additionalProperties": False,
            },
        },
    },
    reasoning_effort="high",
)

```


```python
response.choices[0].message.content
```




    '{\n  "text": "Assistant, please ask the user: \\"The search results show a variety of Gartner reviews for different platforms like enterprise conversational AI, endpoint protection, unified endpoint management, and more. What specific details or insights are you looking for? Are you interested in an overview summary, a detailed comparison of these platforms, or additional information on a particular category? Please clarify your objectives.\\"",\n  "action": "questions",\n  "respond_to_user": false\n}'




```python
json.loads(response.choices[0].message.content)
```




    {'text': 'Assistant, please ask the user: "The search results show a variety of Gartner reviews for different platforms like enterprise conversational AI, endpoint protection, unified endpoint management, and more. What specific details or insights are you looking for? Are you interested in an overview summary, a detailed comparison of these platforms, or additional information on a particular category? Please clarify your objectives."',
     'action': 'questions',
     'respond_to_user': False}




```python

```

================
File: synthesis.py
================
from openai import OpenAI
from config import config
import logging
import json

client = OpenAI(api_key=config["openai_api_key"])


class SynthesisAgent:
    """Agent that synthesizes retrieved data into a coherent summary."""

    def synthesize(self, query: str, retrieval_results: list, iteration: int) -> str:
        """
        Generate a summary based on retrieved data using OpenAI.

        Args:
            query (str): The user's query.
            retrieval_results (list): List of retrieval results, each with 'type' and data.
            iteration (int): Current synthesis iteration number.

        Returns:
            str: JSON string containing 'status', 'summary', and optionally 'additional_tasks'.
        """
        # Format the retrieved data into the prompt
        data_str = ""
        for idx, retrieval in enumerate(retrieval_results, 1):
            if retrieval["type"] == "search":
                data_str += (
                    f"[Source {idx}] Search Results for '{retrieval['query']}':\n"
                )
                for item in retrieval["results"]["items"]:
                    data_str += (
                        f"- {item['title']} ({item['link']})\n  {item['snippet']}\n"
                    )
                data_str += "\n"
            elif retrieval["type"] == "scrape":
                data_str += f"[Source {idx}] Scraped Content from {retrieval['url']}:\n{retrieval['content']}\n\n"

        # Define the synthesis prompt
        prompt = (
            f"This is synthesis iteration {iteration}. If iteration >= 3, generate the best possible summary with available data, noting any missing information.\n"
            f"Based on the following information, provide a detailed summary for the query: '{query}'\n\n"
            f"Data:\n{data_str}\n"
            "Respond with a JSON object containing 'status' ('complete' or 'incomplete'), 'summary', and if 'incomplete', 'additional_tasks'.\n"
            "For 'complete', format the summary in Markdown with clear headings and bullet points. For queries asking for lists (e.g., players over 30), extract and list specific items with details (e.g., names, ages calculated as of February 24, 2025). Include citations [1], [2], etc., corresponding to the sources.\n"
            "For 'incomplete', suggest tasks like {'tool': 'scrape_urls', 'parameters': {'urls': ['url'], 'use_cache': true}} or {'tool': 'google_search', 'parameters': {'q': 'query'}} to gather missing data."
        )

        # Log the input prompt for debugging
        logging.info(f"Synthesis input prompt:\n{prompt}")

        try:
            # Call the OpenAI API
            response = client.chat.completions.create(
                model=config["model"],
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"},
                max_tokens=5000,
            )
            synthesis_output = json.loads(response.choices[0].message.content)
            logging.info(f"Synthesized output for query: {query}")
            return json.dumps(synthesis_output)
        except Exception as e:
            logging.error(f"Synthesis failed for query '{query}': {e}")
            raise Exception(f"Synthesis failed: {e}")

================
File: tool_dispatcher.py
================
import json
from tools.scrape_urls import scrape_urls_call
from tools.google_search import google_search_call
from tools.switch_personas import switch_personas_call


def scratchpad_create_entry(title, content, scratchpad_state=None):
    """
    Creates a new note/block in the scratchpad_state dictionary.
    """
    if scratchpad_state is None:
        scratchpad_state = {}
    entry_id = len(scratchpad_state) + 1
    scratchpad_state[entry_id] = {"title": title, "content": content}
    return f"Created new scratchpad entry #{entry_id} with title '{title}'."


def scratchpad_delete_entry(entry_id, scratchpad_state=None):
    """
    Deletes a note/block by ID from the scratchpad_state dictionary.
    """
    if scratchpad_state is None or entry_id not in scratchpad_state:
        return f"Error: entry_id '{entry_id}' not found."
    del scratchpad_state[entry_id]
    return f"Deleted scratchpad entry #{entry_id}."


def scratchpad_replace_entry(entry_id, content, scratchpad_state=None):
    """
    Replaces or updates the content of a note (block), identified by ID.
    """
    if scratchpad_state is None or entry_id not in scratchpad_state:
        return f"Error: entry_id '{entry_id}' not found."
    scratchpad_state[entry_id]["content"] = content
    return f"Updated scratchpad entry #{entry_id}."


TOOL_MAP = {
    "scrape_urls": scrape_urls_call,
    "google_search": google_search_call,
    "switch_personas": switch_personas_call,
}


def dispatch_tool_call(
    tool_call, conversation=None, thinking_level=None, scratchpad_state=None
):
    """
    Main dispatcher for tool calls. Routes scratchpad calls or falls back to TOOL_MAP.

    Args:
        tool_call (dict): The tool call details with id and function.
        conversation (Conversation, optional): Conversation context for switch_personas.
        thinking_level (str, optional): Thinking level for switch_personas.
        scratchpad_state (dict, optional): Scratchpad state for persistence.

    Returns:
        Any: Result of the tool call execution.
    """
    function_name = tool_call["function"]["name"]
    arguments = tool_call["function"]["arguments"]

    # Handle scratchpad calls
    if function_name == "scratchpad_create_entry":
        title = arguments.get("title", "Untitled")
        content = arguments.get("content", "")
        return scratchpad_create_entry(title, content, scratchpad_state)

    elif function_name == "scratchpad_delete_entry":
        entry_id = arguments.get("entry_id", 0)
        return scratchpad_delete_entry(entry_id, scratchpad_state)

    elif function_name == "scratchpad_replace_entry":
        entry_id = arguments.get("entry_id", 0)
        new_content = arguments.get("content", "")
        return scratchpad_replace_entry(entry_id, new_content, scratchpad_state)

    # Fallback to known tools
    if function_name not in TOOL_MAP:
        raise ValueError(f"Unknown tool: {function_name}")

    handler = TOOL_MAP[function_name]

    if function_name == "switch_personas":
        return handler(
            switch_to=arguments["switch_to"],
            conversation=conversation,
            thinking_level=thinking_level or "high",
        )

    return handler(**arguments)

================
File: utils.py
================
import logging
from rich.logging import RichHandler


def setup_logging(verbose=False):
    """Set up logging configuration with RichHandler."""
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format="%(message)s",
        datefmt="[%X]",
        handlers=[RichHandler(rich_tracebacks=True)],
    )
    logger = logging.getLogger("research_bot")
    logger.setLevel(level)
    return logger

================
File: validation.py
================
from openai import OpenAI
from config import config
import logging
import tiktoken

client = OpenAI(api_key=config["openai_api_key"])


class ValidationAgent:
    def validate(self, summary: str, sources: str) -> str:
        """Verify that factual claims in the summary have citations, using multiple passes if needed."""
        # Token limit for each chunk (leave buffer for prompt)
        MAX_TOKENS_PER_PASS = 100000
        encoding = tiktoken.encoding_for_model(config["model"])

        # Split summary into chunks
        summary_tokens = encoding.encode(summary)
        chunks = []
        current_chunk = []
        current_count = 0

        for token in summary_tokens:
            if current_count + 1 > MAX_TOKENS_PER_PASS:
                chunks.append(encoding.decode(current_chunk))
                current_chunk = [token]
                current_count = 1
            else:
                current_chunk.append(token)
                current_count += 1
        if current_chunk:
            chunks.append(encoding.decode(current_chunk))

        logging.info(f"Split summary into {len(chunks)} chunks for validation.")

        # Validate each chunk
        reports = []
        for i, chunk in enumerate(chunks, 1):
            prompt = (
                f"Check the following summary chunk for factual claims without citations. "
                f"A factual claim is any statement about players, teams, or ages that isn’t "
                f"general knowledge (e.g., 'The sky is blue'). Each claim should have a citation "
                f"like [n]. List any uncited claims in Markdown. If all claims are cited, say 'All claims cited.'\n\n"
                f"Chunk {i}:\n{chunk}"
            )
            try:
                response = client.chat.completions.create(
                    model=config["model"],
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=2000,  # Adjust based on expected output size
                )
                report = response.choices[0].message.content
                reports.append(f"### Chunk {i} Validation\n{report}")
            except Exception as e:
                logging.error(f"Validation failed for chunk {i}: {e}")
                reports.append(f"### Chunk {i} Validation\nError: {str(e)}")

        # Combine reports
        final_report = "\n\n".join(reports)
        logging.info("Generated multi-pass validation report")
        return final_report
