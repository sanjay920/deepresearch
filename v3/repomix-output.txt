Review the following:

### Why DeepSearch Should Be a Tool in `cli.py`

Making DeepSearch a tool within `cli.py`—in addition to its role as the main AI assistant—would supercharge the system’s ability to handle complex, research-heavy tasks. Imagine a user asking for a 50-page report on AI agent platforms: DeepSearch could break it down, delegate parts to itself, and stitch it all together. Here’s why this is a great idea:

- **Recursive Power**: DeepSearch could call itself to tackle sub-tasks. For that 50-page report, it might assign one instance to research market trends, another to compare platforms, and a third to gather user feedback—each using tools like `google_search` or `web_research`. This divide-and-conquer approach mimics how humans manage big projects.

- **Modularity**: As a tool, DeepSearch becomes a reusable component. The main agent (or orchestrator) can decide when to tap into its research skills, making the system more flexible and scalable. Need heavy research? Spin up a few DeepSearch instances to handle it in parallel.

- **Specialization**: DeepSearch is a research and synthesis pro. Letting other parts of the system call it as a tool means they don’t have to reinvent that wheel—it’s like having a dedicated librarian on speed dial.

- **Clean Context**: Each DeepSearch instance can keep its own context, so sub-tasks don’t get muddled with irrelevant info. This keeps things clear and efficient, especially for multi-step queries.

- **Bigger Capabilities**: With this setup, the system could take on ambitious tasks—like that 50-page report—by autonomously gathering data, analyzing it, and presenting it in a polished format.

- **Flexibility**: The orchestrator can mix and match DeepSearch with other tools or handle simpler tasks itself. This optimizes speed and resources, making the system both powerful and practical.

In short, turning DeepSearch into a tool makes it a Swiss Army knife for research, unlocking new levels of productivity and complexity-handling.

---

### What Needs to Be Refactored

To pull this off, some tweaks to the system are necessary. Here’s what would need to change:

1. **Define DeepSearch as a Tool**  
   - Add DeepSearch to `config.py` as a tool, like `google_search`. Give it a name, a description (e.g., "Handles research and sub-tasks"), and an input schema (e.g., a query string and optional context).

2. **Set Up Invocation**  
   - Create a function in `cli.py`—say, `deepsearch_tool(query, context)`—to call DeepSearch. This lets the orchestrator trigger it like any other tool, passing in what it needs to work on.

3. **Pass Context Smartly**  
   - Update the `Conversation` class to send the right context to DeepSearch when it’s called. This might mean pulling out just the relevant history or task details for each sub-task.

4. **Control Recursion**  
   - Add a limit to how many times DeepSearch can call itself (e.g., a max depth or call tracker) to avoid infinite loops or resource hogging.

5. **Standardize Output**  
   - Make DeepSearch return results in a consistent format—like a JSON with a `result` field—so the orchestrator can easily use or combine them.

6. **Manage Tokens**  
   - Since every call eats tokens, set budgets for sub-tasks or trim context to stay under limits. This keeps costs and performance in check.

7. **Handle Errors**  
   - Beef up error handling so if a sub-task flops, the orchestrator can retry, switch tactics, or tell the user without crashing.

8. **Keep Tool Access**  
   - Ensure DeepSearch, as a tool, can still use `google_search`, `web_research`, etc. This might mean passing tool definitions to each instance or making them globally available.

9. **Tweak the Interface**  
   - Adjust `cli.py` to show when DeepSearch is running as a tool (e.g., different text formatting or logs) so users know what’s happening.

10. **Test It Out**  
    - Run thorough tests to confirm recursion works, context stays clean, and big tasks don’t tank performance or blow token budgets.

---

### Wrap-Up

Making DeepSearch a tool in `cli.py` is a game-changer—it’s like giving the system a research superpower with recursive smarts and modularity. The refactoring isn’t trivial (context, recursion, and tokens need careful handling), but the payoff is huge: a system that can autonomously crush complex tasks like a 50-page report. With these changes, DeepSearch could evolve from a helper to a cornerstone of something truly powerful.

Here is an example run:
(deepresearch-private-py3.11) (base) sanjay_acorn@Mac v3 % python cli.py    
╭── Welcome ──╮
│ Deep Search │
╰─────────────╯
Type your messages or 'exit' to quit
Commands: thinking:on, thinking:off, clear

You: how were nvidia earnings from today?
[User message tokens: 8]
Message Received

Thinking...
The user is asking about NVIDIA's earnings that were reported today. This is a request for current financial information that would require up-to-date data. I should use the google_search tool to look up the latest information about NVIDIA's earnings since this would be recent financial news.

I'll make a search query about NVIDIA's most recent earnings.

Thinking completed in 00:01

--------------------------------------------------------------------------------


Assistant:
I'll look up the latest information about NVIDIA's earnings for you.

Using tool: google_search


[Tokens: 109 total = 8 (user) + 14 (assistant) + 87 (overhead)]

Executing tools...
Executing tool: google_search
Searching for: nvidia earnings results february 2025
Tool result added to conversation

Continuing with tool results...

Assistant:


Let me check for the most recent earnings announcement from today:

Using tool: web_research


[Tokens: 2664 total = 2408 (user) + 13 (assistant) + 243 (overhead)]

Executing tools...
Executing tool: web_research
Researching: What were NVIDIA's Q4 fiscal 2025 earnings released today? Include revenue, profit, and any major highlights or guidance on 
http://nvidianews.nvidia.com/news/nvidia-announces-financial-results-for-fourth-quarter-and-fiscal-2025
Tool result added to conversation

Continuing with tool results...

Assistant:


Based on the latest information, I can provide you with details about NVIDIA's earnings released today (February 26, 2025).

## NVIDIA Q4 Fiscal 2025 Earnings Results

NVIDIA reported exceptional results for their fourth quarter and full fiscal year 2025 (ended January 26, 2025) [1]:

### Key Financial Highlights:

- **Record quarterly revenue**: $39.3 billion, up 12% from previous quarter and 78% year-over-year [1]
- **Record Data Center revenue**: $35.6 billion, up 16% from previous quarter and 93% year-over-year [1]
- **GAAP earnings per diluted share**: $0.89, up 14% from previous quarter and 82% year-over-year [1]
- **Non-GAAP earnings per diluted share**: $0.89, up 10% from previous quarter and 71% year-over-year [1]

### Full Fiscal Year 2025 Performance:

- **Record annual revenue**: $130.5 billion, up 114% from previous fiscal year [1]

### Business Highlights:

- Strong demand for **Blackwell AI supercomputers** with "billions of dollars in sales in its first quarter" [1]
- Launch of new **GeForce RTX 50 Series** graphics cards and laptops powered by the Blackwell architecture [1]
- New automotive partnerships with **Toyota** (using NVIDIA DRIVE AGX Orin platform) and **Hyundai Motor Group** (enhancing vehicle safety with AI and Omniverse) [1]

### Forward Guidance:

For Q1 Fiscal 2026, NVIDIA expects [1]:
- **Revenue**: $43.0 billion (±2%)
- **GAAP gross margin**: 70.6% (±0.5%)
- **Non-GAAP gross margin**: 71.0% (±0.5%)

CEO Jensen Huang highlighted the "amazing" demand for Blackwell AI supercomputers, attributing it to growing adoption of reasoning AI technologies [1].

References:
[1] NVIDIA News - NVIDIA Announces Financial Results for Fourth Quarter and Fiscal 2025 - http://nvidianews.nvidia.com/news/nvidia-announces-financial-results-for-fourth-quarter-and-fiscal-2025

[Tokens: 4189 total = 978 (user) + 475 (assistant) + 2736 (overhead)]

---

================================================================
Directory Structure
================================================================
workspace/
  test1.md
  test2.md
cli.py
config.py
conversation.py
requirements.txt
tools.py

================================================================
Files
================================================================

================
File: workspace/test1.md
================
# section1

# section 0

(Initial content for section 0)

# sectoin1 

(Initial content for section1)

================
File: workspace/test2.md
================
# section 0

(Initial content for section 0)

# sectoin1 

(Initial content for section1)

================
File: cli.py
================
#!/usr/bin/env python3
import anthropic
import sys
import logging
import time
import threading
import os
import json
from rich.console import Console
from rich.panel import Panel
from rich.align import Align
from conversation import Conversation
from config import CONFIG, SYSTEM_PROMPT, TOOLS
from tools import (
    google_search,
    web_research,
    notion_agent,
)
from rich.spinner import Spinner
from rich.live import Live
import tiktoken

# Setup console and logging
console = Console()
logging.basicConfig(
    level=logging.INFO,  # Changed to INFO to see tool execution logs
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("claude_cli.log")
    ],  # Send logs to a file instead of stdout
)


# Initialize tiktoken encoder for token counting
def get_encoder():
    """Get the appropriate tiktoken encoder based on the model."""
    model = CONFIG["model"]
    try:
        if "claude" in model.lower():
            # For Claude models, use cl100k_base which is close to Claude's tokenizer
            return tiktoken.get_encoding("cl100k_base")
        else:
            # Try to get model-specific encoding, fallback to cl100k_base
            return tiktoken.encoding_for_model(model)
    except:
        # Fallback to cl100k_base if specific encoding not found
        return tiktoken.get_encoding("cl100k_base")


def count_tokens(text):
    """Count the number of tokens in a text string or content blocks."""
    encoder = get_encoder()

    # Handle different content types
    if text is None:
        return 0
    elif isinstance(text, str):
        # Simple string case
        return len(encoder.encode(text))
    elif isinstance(text, list):
        # Content blocks case (from tool results)
        logging.info(f"Counting tokens for content blocks (list of {len(text)} items)")
        # Convert the content blocks to JSON and count tokens
        return len(encoder.encode(json.dumps(text)))
    else:
        # Unexpected type
        logging.warning(
            f"Unexpected content type in count_tokens: {type(text).__name__}"
        )
        # Try to convert to string as fallback
        try:
            return len(encoder.encode(str(text)))
        except Exception as e:
            logging.error(f"Failed to count tokens: {str(e)}")
            return 0  # Return 0 as fallback


def execute_tool_call(tool_call):
    """
    Execute a tool call based on its name and input.

    Args:
        tool_call: Dictionary with tool details and input

    Returns:
        The result of the tool execution
    """
    tool_name = tool_call["name"]
    tool_input = tool_call["input"]

    console.print(f"[bold yellow]Executing tool:[/bold yellow] {tool_name}")
    logging.info(f"Starting tool execution: {tool_name}")
    logging.info(f"Tool input: {tool_input}")

    try:
        if tool_name == "google_search":
            query = tool_input.get("query", tool_input.get("q", ""))
            if not query:
                console.print("[bold red]Error:[/bold red] Empty search query")
                logging.error("Empty search query provided to google_search tool")
                return {"error": "Empty search query"}

            console.print(f"[dim]Searching for:[/dim] {query}")
            logging.info(f"Google search query: {query}")
            result = google_search(query)
            # Break long line into multiple lines
            msg = "Google search completed. "
            msg += f"Result type: {type(result).__name__}"
            logging.info(msg)
            logging.debug(f"Google search result: {result}")
            return result

        elif tool_name == "web_research":
            url = tool_input.get("url", "")
            query = tool_input.get("query", "")
            console.print(f"[dim]Researching:[/dim] {query} [dim]on[/dim] {url}")
            logging.info(f"Web research: query='{query}', url='{url}'")
            try:
                result = web_research(url, query)
                # Break long line into multiple lines
                msg = "Web research completed. "
                msg += f"Result type: {type(result).__name__}"
                logging.info(msg)
                # Ensure result is a dictionary before converting to JSON
                if not isinstance(result, dict):
                    # Break long line into multiple lines
                    msg = "Web research returned non-dict result: "
                    msg += f"{type(result).__name__}"
                    logging.warning(msg)
                    result = {
                        "error": "Invalid response format",
                        "raw_result": str(result),
                    }
                logging.info(
                    f"Web research completed. Result type: {type(result).__name__}"
                )
                logging.debug(f"Web research result: {result}")
                return result
            except Exception as e:
                error_msg = f"Web research error: {str(e)}"
                logging.error(error_msg)
                logging.error(f"Exception type: {type(e).__name__}", exc_info=True)
                return {"error": error_msg}

        elif tool_name == "notion_agent":
            query = tool_input.get("query", "")
            if not query:
                console.print("[bold red]Error:[/bold red] Empty Notion query")
                return {"error": "Empty Notion query"}

            console.print(f"[dim]Processing Notion operation:[/dim] {query}")
            result = notion_agent(query)

            # Log the result for debugging
            logging.info(f"Notion agent result: {result}")

            # If there's an error, display it
            if "error" in result:
                console.print(
                    f"[bold red]Notion agent error:[/bold red] {result['error']}"
                )

            return result

        # Keep workspace_agent for backward compatibility
        elif tool_name == "workspace_agent":
            query = tool_input.get("query", "")
            if not query:
                console.print("[bold red]Error:[/bold red] Empty workspace query")
                return {"error": "Empty workspace query"}

            console.print(f"[dim]Processing document operation:[/dim] {query}")
            return notion_agent(query)  # Redirect to notion_agent

        else:
            error_msg = f"Unknown tool: {tool_name}"
            logging.error(error_msg)
            return {"error": error_msg}

    except Exception as e:
        error_msg = f"Tool execution error: {str(e)}"
        logging.error(error_msg)
        return {"error": error_msg}


def main():
    """
    Main CLI interface for conversing with Claude and displaying thinking.

    Features:
    - Token management: Automatically trims thinking blocks when conversation
      exceeds 80k tokens
    - Empty response handling: Prevents API errors by replacing empty responses
      with placeholder text
    - Tool execution: Supports calling external tools and continuing the
      conversation with results
    - Thinking display: Shows Claude's thinking process (can be toggled on/off)
    """
    console.print(
        Panel.fit(
            "[bold blue]Deep Search[/bold blue]",
            title="Welcome",
        )
    )
    console.print("Type your messages or [blue]'exit'[/blue] to quit")
    console.print(
        "Commands: [blue]thinking:on[/blue], [blue]thinking:off[/blue], "
        "[blue]clear[/blue]"
    )

    # Initialize Anthropic client
    client = anthropic.Anthropic(api_key=CONFIG["anthropic_api_key"])
    conversation = Conversation()
    show_thinking = True  # Default to showing thinking

    while True:
        user_input = console.input("\n[bold green]You:[/bold green] ")

        # Command handling
        if user_input.lower() in ["exit", "quit"]:
            console.print("[bold blue]Goodbye![/bold blue]")
            break

        if user_input.lower() == "thinking:on":
            show_thinking = True
            console.print("[bold blue]Thinking display turned ON[/bold blue]")
            continue

        elif user_input.lower() == "thinking:off":
            show_thinking = False
            console.print("[bold blue]Thinking display turned OFF[/bold blue]")
            continue

        elif user_input.lower() == "clear":
            conversation.clear()
            console.print("[bold blue]Conversation history cleared[/bold blue]")
            continue

        # Count tokens in user message
        user_tokens = count_tokens(user_input)
        console.print(f"[cyan][User message tokens: {user_tokens}][/cyan]")

        # Add user message to conversation without timestamp
        conversation.add_user_message(user_input)

        # Show a simple "message received" notice
        console.print("[bold yellow]Message Received[/bold yellow]")

        # Process streaming conversation with tool calls
        process_conversation(client, conversation, show_thinking)


def trim_thinking_tokens(conversation, max_tokens=80000):
    """
    Trim thinking tokens from the conversation history when it exceeds max_tokens.
    Removes the first 5 thinking blocks to reduce context size.

    Args:
        conversation: Conversation history manager
        max_tokens: Maximum token threshold before trimming

    Returns:
        bool: True if trimming was performed, False otherwise
    """
    # Get all messages and count tokens
    all_messages = conversation.get_messages()
    conversation_text = json.dumps(all_messages)
    total_tokens = count_tokens(conversation_text)

    if total_tokens < max_tokens:
        return False  # No trimming needed

    logging.info(
        f"Conversation exceeds {max_tokens} tokens ({total_tokens}). Trimming thinking blocks..."
    )

    # Find and remove the first 5 thinking blocks
    thinking_blocks_removed = 0
    for message in all_messages:
        if message["role"] == "assistant" and "content" in message:
            # Create a new content list without some thinking blocks
            new_content = []
            for block in message["content"]:
                if (
                    block["type"] in ["thinking", "redacted_thinking"]
                    and thinking_blocks_removed < 5
                ):
                    thinking_blocks_removed += 1
                    logging.info(f"Removed thinking block {thinking_blocks_removed}")
                    continue  # Skip this thinking block
                new_content.append(block)

            # Replace the content with trimmed version
            message["content"] = new_content

            if thinking_blocks_removed >= 5:
                break  # Stop after removing 5 blocks

    # Also trim tool results if needed
    if thinking_blocks_removed < 5:
        for message in all_messages:
            if (
                message["role"] == "user"
                and "content" in message
                and isinstance(message["content"], list)
            ):
                new_content = []
                for block in message["content"]:
                    if block["type"] == "tool_result" and thinking_blocks_removed < 5:
                        thinking_blocks_removed += 1
                        logging.info(
                            f"Removed tool result block {thinking_blocks_removed}"
                        )
                        continue  # Skip this tool result
                    new_content.append(block)

                message["content"] = new_content

                if thinking_blocks_removed >= 5:
                    break  # Stop after removing 5 blocks

    # Update the conversation with trimmed messages
    conversation.messages = all_messages

    # Log the new token count
    new_conversation_text = json.dumps(conversation.get_messages())
    new_total_tokens = count_tokens(new_conversation_text)
    logging.info(
        f"After trimming: {new_total_tokens} tokens (saved {total_tokens - new_total_tokens} tokens)"
    )

    return True


def process_conversation(client, conversation, show_thinking):
    """
    Process a complete conversation turn, handling thinking and tools.

    Args:
        client: Anthropic client
        conversation: Conversation history manager
        show_thinking: Whether to display thinking blocks
    """
    # Check if we need to trim thinking tokens
    trim_thinking_tokens(conversation)

    # Initialize tracking variables
    all_content_blocks = []
    thinking_start_time = None
    thinking_started = False
    response_started = False
    tool_calls = []
    is_tool_use = False
    assistant_response_text = ""  # Track assistant's text response for token counting
    empty_response = True  # Track if response contains only whitespace

    # Get the user's message token count from the last message
    last_user_message = None
    for message in reversed(conversation.get_messages()):
        if message["role"] == "user":
            last_user_message = message["content"]
            break

    user_tokens = count_tokens(last_user_message) if last_user_message else 0

    # Add a variable to track partial JSON for tool inputs
    tool_input_json_parts = {}

    try:
        # Create a spinner to show while waiting for the first response
        spinner = Spinner("dots", text="Waiting for assistant...")

        # Get current timestamp for this request
        current_timestamp = time.strftime("%Y-%m-%d %H:%M:%S")

        # Create a timestamped system prompt
        timestamped_system_prompt = (
            f"{SYSTEM_PROMPT}\n\nCurrent time: {current_timestamp}"
        )

        # Start the spinner in a Live context
        with Live(spinner, refresh_per_second=10, transient=True) as live:
            # Stream the response from Claude
            with client.messages.stream(
                model=CONFIG["model"],
                max_tokens=CONFIG["max_tokens"],
                system=timestamped_system_prompt,  # Use timestamped system prompt
                messages=conversation.get_messages(),
                temperature=CONFIG["temperature"],
                thinking={
                    "type": "enabled",
                    "budget_tokens": CONFIG["thinking"]["budget_tokens"],
                },
                tools=TOOLS,
            ) as stream:
                # Track content blocks for conversation history
                current_block = None

                for event in stream:
                    # Stop the spinner on first event
                    if live.is_started:
                        live.stop()

                    if event.type == "content_block_start":
                        block_type = event.content_block.type
                        current_block = {"type": block_type}

                        if block_type == "thinking":
                            current_block["thinking"] = ""
                            if show_thinking and not thinking_started:
                                thinking_start_time = time.time()
                                console.print("\n[bold blue]Thinking...[/bold blue]")
                                thinking_started = True

                        elif block_type == "redacted_thinking":
                            current_block["data"] = ""
                            if show_thinking and not thinking_started:
                                console.print(
                                    "\n[bold red]Redacted Thinking:[/bold red]"
                                )
                                console.print(
                                    "[red]Some thinking content was filtered for safety[/red]"
                                )
                                thinking_start_time = time.time()
                                thinking_started = True

                        elif block_type == "text":
                            current_block["text"] = ""
                            if not response_started:
                                # Show thinking time if applicable
                                if thinking_start_time:
                                    elapsed = time.time() - thinking_start_time
                                    minutes, seconds = divmod(int(elapsed), 60)
                                    console.print(
                                        f"\n[bold yellow]Thinking completed in {minutes:02d}:{seconds:02d}[/bold yellow]"
                                    )

                                # Add a separator between thinking and response
                                if show_thinking and thinking_started:
                                    console.print("\n" + "-" * 80 + "\n")
                                console.print("\n[bold green]Assistant:[/bold green]")
                                response_started = True

                        elif block_type == "tool_use":
                            is_tool_use = True
                            current_block["id"] = event.content_block.id
                            current_block["name"] = event.content_block.name
                            current_block["input"] = event.content_block.input

                            # More detailed logging to understand the structure
                            logging.info(
                                f"Tool call detected: {event.content_block.name}"
                            )
                            logging.info(f"Tool call ID: {event.content_block.id}")
                            logging.info(
                                f"Tool call input: {event.content_block.input}"
                            )
                            logging.info(f"Full content block: {event.content_block}")

                            # Only add to tool_calls if we have input parameters
                            if event.content_block.input:
                                tool_calls.append(
                                    {
                                        "id": event.content_block.id,
                                        "name": event.content_block.name,
                                        "input": event.content_block.input,
                                    }
                                )
                            else:
                                # If input is empty, wait for potential updates in content_block_delta events
                                logging.warning(
                                    f"Empty input for tool {event.content_block.name}. Will wait for updates."
                                )

                            # Show tool use in console
                            if not response_started:
                                if thinking_start_time:
                                    elapsed = time.time() - thinking_start_time
                                    minutes, seconds = divmod(int(elapsed), 60)
                                    console.print(
                                        f"\n[bold yellow]Thinking completed in {minutes:02d}:{seconds:02d}[/bold yellow]"
                                    )

                                if show_thinking and thinking_started:
                                    console.print("\n" + "-" * 80 + "\n")
                                console.print("\n[bold green]Assistant:[/bold green]")
                                response_started = True

                            console.print(
                                f"\n[bold magenta]Using tool:[/bold magenta] {event.content_block.name}"
                            )

                    elif event.type == "content_block_delta":
                        delta_type = event.delta.type

                        # Log all delta types to understand what's coming through
                        logging.info(f"Delta type: {delta_type}")
                        logging.info(f"Delta content: {event.delta}")

                        if delta_type == "thinking_delta":
                            current_block["thinking"] += event.delta.thinking
                            if show_thinking:
                                console.print(
                                    event.delta.thinking, end="", highlight=False
                                )
                                sys.stdout.flush()

                        elif delta_type == "text_delta":
                            # Check if the text contains only whitespace
                            text_content = event.delta.text
                            if text_content.strip():
                                empty_response = False  # Contains non-whitespace

                            current_block["text"] += text_content
                            assistant_response_text += (
                                text_content  # Track for token counting
                            )
                            console.print(text_content, end="", highlight=False)
                            sys.stdout.flush()

                        elif delta_type == "input_json_delta":
                            # Handle partial JSON for tool inputs
                            if current_block and current_block["type"] == "tool_use":
                                tool_id = current_block.get("id")
                                if tool_id not in tool_input_json_parts:
                                    tool_input_json_parts[tool_id] = ""

                                # Append the partial JSON
                                if hasattr(event.delta, "partial_json"):
                                    tool_input_json_parts[
                                        tool_id
                                    ] += event.delta.partial_json

                                    # Try to parse the JSON if it looks complete
                                    json_str = tool_input_json_parts[tool_id]
                                    if json_str and (
                                        json_str.startswith("{")
                                        and json_str.endswith("}")
                                    ):
                                        try:
                                            # Parse the complete JSON
                                            logging.info(
                                                f"Attempting to parse JSON: {json_str}"
                                            )
                                            input_params = json.loads(json_str)
                                            logging.info(
                                                f"Parsed tool input: {input_params}"
                                            )

                                            # Update the current block
                                            current_block["input"] = input_params

                                            # Update or add to tool_calls
                                            tool_call_exists = False
                                            for tool_call in tool_calls:
                                                if tool_call["id"] == tool_id:
                                                    tool_call["input"] = input_params
                                                    tool_call_exists = True
                                                    break

                                            if not tool_call_exists:
                                                tool_calls.append(
                                                    {
                                                        "id": tool_id,
                                                        "name": current_block["name"],
                                                        "input": input_params,
                                                    }
                                                )

                                        except json.JSONDecodeError as json_err:
                                            # JSON is not complete yet, continue collecting
                                            logging.info(
                                                f"Partial JSON not yet complete: {json_str}"
                                            )
                                            logging.info(
                                                f"JSONDecodeError: {str(json_err)}"
                                            )
                                        except Exception as e:
                                            # Log any other exceptions during JSON parsing
                                            logging.error(
                                                f"Error parsing JSON: {str(e)}"
                                            )
                                            logging.error(f"JSON string: {json_str}")
                                            logging.error(
                                                f"Exception type: {type(e).__name__}"
                                            )

                        elif delta_type == "signature_delta":
                            current_block["signature"] = event.delta.signature

                    elif event.type == "content_block_stop":
                        if current_block:
                            # If this is a text block and contains only whitespace, add a placeholder
                            if (
                                current_block["type"] == "text"
                                and empty_response
                                and not is_tool_use
                            ):
                                current_block["text"] = "I'm processing your request."
                                console.print(
                                    "\nAdded placeholder text to prevent empty response error.",
                                    style="yellow",
                                )
                                empty_response = False

                            all_content_blocks.append(current_block)
                            current_block = None
                            # Add a newline after each content block
                            console.print()

                # Add assistant's response to conversation history
                conversation.add_assistant_message(all_content_blocks)

                # Count tokens in assistant's response
                assistant_tokens = count_tokens(assistant_response_text)

                # Count tokens in visible conversation (excluding thinking blocks)
                if hasattr(conversation, "get_visible_messages"):
                    visible_conversation_text = json.dumps(
                        conversation.get_visible_messages()
                    )
                    visible_conversation_tokens = count_tokens(
                        visible_conversation_text
                    )
                    overhead_tokens = (
                        visible_conversation_tokens - user_tokens - assistant_tokens
                    )
                    total_tokens = user_tokens + assistant_tokens + overhead_tokens
                else:
                    # Count tokens in entire conversation
                    conversation_text = json.dumps(conversation.get_messages())
                    conversation_tokens = count_tokens(conversation_text)
                    overhead_tokens = (
                        conversation_tokens - user_tokens - assistant_tokens
                    )
                    total_tokens = user_tokens + assistant_tokens + overhead_tokens

                # Display the total and breakdown
                console.print(
                    f"\n[cyan][Tokens: {total_tokens} total = {user_tokens} (user) + {assistant_tokens} (assistant) + {overhead_tokens} (overhead)][/cyan]"
                )

                # Display token usage if available
                if hasattr(stream, "usage") and stream.usage:
                    input_tokens = stream.usage.input_tokens
                    output_tokens = stream.usage.output_tokens
                    console.print(
                        f"[dim]API usage: {input_tokens} input, {output_tokens} output[/dim]"
                    )

        # Handle tool calls if any
        if is_tool_use:
            console.print("\n[bold yellow]Executing tools...[/bold yellow]")

            # Execute each tool call and add results to conversation
            for tool_call in tool_calls:
                logging.info(
                    f"Executing tool call: {tool_call['name']} (ID: {tool_call['id']})"
                )
                result = execute_tool_call(tool_call)
                logging.info(f"Tool result type: {type(result).__name__}")

                try:
                    # Log the result before converting to JSON
                    logging.debug(f"Raw tool result: {result}")
                    result_str = json.dumps(result)
                    logging.info(
                        f"Converted tool result to JSON string (length: {len(result_str)})"
                    )
                except Exception as e:
                    logging.error(f"Error converting tool result to JSON: {str(e)}")
                    logging.error(f"Exception type: {type(e).__name__}")
                    logging.error(f"Result that failed conversion: {result}")
                    # Provide a fallback result
                    result_str = json.dumps(
                        {"error": f"Failed to convert result: {str(e)}"}
                    )
                    logging.info("Using fallback error result")

                # FIXED: Only add the tool result without thinking blocks
                logging.info(
                    f"Adding tool result to conversation for ID: {tool_call['id']}"
                )
                conversation.add_tool_result(tool_call["id"], result_str)
                console.print(f"[dim]Tool result added to conversation[/dim]")

            # Continue the conversation with new information
            console.print(
                "\n[bold yellow]Continuing with tool results...[/bold yellow]"
            )

            # Check if we need to trim thinking tokens before continuing
            trim_thinking_tokens(conversation)

            # IMPORTANT: Disable thinking for the tool result response
            # This is the key fix - we need to disable thinking when continuing after tool results
            process_conversation_without_thinking(client, conversation, show_thinking)

    except Exception as e:
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        logging.error(f"API error: {str(e)}")
        logging.error(f"Exception type: {type(e).__name__}")
        logging.error(f"Exception details:", exc_info=True)


def process_conversation_without_thinking(client, conversation, show_thinking):
    """
    Process a conversation turn after tool results without using thinking.

    Args:
        client: Anthropic client
        conversation: Conversation history manager
        show_thinking: Whether to display thinking (not used in this function)
    """
    # Check if we need to trim thinking tokens before continuing
    trim_thinking_tokens(conversation)

    try:
        # Create a spinner to show while waiting for the first response
        spinner = Spinner("dots", text="Processing tool results...")
        assistant_response_text = (
            ""  # Track assistant's text response for token counting
        )
        empty_response = True  # Track if response contains only whitespace

        # Get current timestamp for this request
        current_timestamp = time.strftime("%Y-%m-%d %H:%M:%S")

        # Create a timestamped system prompt
        timestamped_system_prompt = (
            f"{SYSTEM_PROMPT}\n\nCurrent time: {current_timestamp}"
        )

        # Start the spinner in a Live context
        with Live(spinner, refresh_per_second=10, transient=True) as live:
            # Stream the response from Claude without thinking enabled
            with client.messages.stream(
                model=CONFIG["model"],
                max_tokens=CONFIG["max_tokens"],
                system=timestamped_system_prompt,  # Use timestamped system prompt
                messages=conversation.get_messages(),
                temperature=CONFIG["temperature"],
                # No thinking parameter here
                tools=TOOLS,
            ) as stream:
                # Track content blocks for conversation history
                all_content_blocks = []
                current_block = None
                response_started = False
                tool_calls = []
                is_tool_use = False
                tool_input_json_parts = {}

                for event in stream:
                    # Stop the spinner on first event
                    if live.is_started:
                        live.stop()

                    if event.type == "content_block_start":
                        block_type = event.content_block.type
                        current_block = {"type": block_type}

                        if block_type == "text":
                            current_block["text"] = ""
                            if not response_started:
                                console.print("\n[bold green]Assistant:[/bold green]")
                                response_started = True

                        elif block_type == "tool_use":
                            is_tool_use = True
                            current_block["id"] = event.content_block.id
                            current_block["name"] = event.content_block.name
                            current_block["input"] = event.content_block.input

                            # Log tool use details
                            logging.info(
                                f"Tool call detected: {event.content_block.name}"
                            )
                            logging.info(f"Tool call ID: {event.content_block.id}")

                            # Add to tool_calls if we have input parameters
                            if event.content_block.input:
                                tool_calls.append(
                                    {
                                        "id": event.content_block.id,
                                        "name": event.content_block.name,
                                        "input": event.content_block.input,
                                    }
                                )

                            # Show tool use in console
                            if not response_started:
                                console.print("\n[bold green]Assistant:[/bold green]")
                                response_started = True

                            console.print(
                                f"\n[bold magenta]Using tool:[/bold magenta] {event.content_block.name}"
                            )

                    elif event.type == "content_block_delta":
                        delta_type = event.delta.type

                        # Log delta types
                        logging.info(f"Delta type: {delta_type}")
                        logging.info(f"Delta content: {event.delta}")

                        if delta_type == "text_delta":
                            # Check if the text contains only whitespace
                            text_content = event.delta.text
                            if text_content.strip():
                                empty_response = False  # Contains non-whitespace

                            current_block["text"] += text_content
                            assistant_response_text += text_content
                            console.print(text_content, end="", highlight=False)
                            sys.stdout.flush()

                        elif delta_type == "input_json_delta":
                            # Handle partial JSON for tool inputs
                            if current_block and current_block["type"] == "tool_use":
                                tool_id = current_block.get("id")
                                if tool_id not in tool_input_json_parts:
                                    tool_input_json_parts[tool_id] = ""

                                # Append the partial JSON
                                if hasattr(event.delta, "partial_json"):
                                    tool_input_json_parts[
                                        tool_id
                                    ] += event.delta.partial_json

                                    # Try to parse the JSON if it looks complete
                                    json_str = tool_input_json_parts[tool_id]
                                    if json_str and (
                                        json_str.startswith("{")
                                        and json_str.endswith("}")
                                    ):
                                        try:
                                            # Parse the complete JSON
                                            logging.info(
                                                f"Attempting to parse JSON: {json_str}"
                                            )
                                            input_params = json.loads(json_str)
                                            logging.info(
                                                f"Parsed tool input: {input_params}"
                                            )

                                            # Update the current block
                                            current_block["input"] = input_params

                                            # Update or add to tool_calls
                                            tool_call_exists = False
                                            for tool_call in tool_calls:
                                                if tool_call["id"] == tool_id:
                                                    tool_call["input"] = input_params
                                                    tool_call_exists = True
                                                    break

                                            if not tool_call_exists:
                                                tool_calls.append(
                                                    {
                                                        "id": tool_id,
                                                        "name": current_block["name"],
                                                        "input": input_params,
                                                    }
                                                )

                                        except json.JSONDecodeError as json_err:
                                            # JSON is not complete yet, continue collecting
                                            logging.info(
                                                f"Partial JSON not yet complete: {json_str}"
                                            )
                                            logging.info(
                                                f"JSONDecodeError: {str(json_err)}"
                                            )
                                        except Exception as e:
                                            # Log any other exceptions during JSON parsing
                                            logging.error(
                                                f"Error parsing JSON: {str(e)}"
                                            )
                                            logging.error(f"JSON string: {json_str}")
                                            logging.error(
                                                f"Exception type: {type(e).__name__}"
                                            )

                    elif event.type == "content_block_stop":
                        if current_block:
                            # If this is a text block and contains only whitespace, add a placeholder
                            if (
                                current_block["type"] == "text"
                                and empty_response
                                and not is_tool_use
                            ):
                                current_block["text"] = "I'm processing your request."
                                console.print(
                                    "\nAdded placeholder text to prevent empty response error.",
                                    style="yellow",
                                )
                                empty_response = False

                            all_content_blocks.append(current_block)
                            current_block = None
                            # Add a newline after each content block
                            console.print()

                # Add assistant's response to conversation history
                conversation.add_assistant_message(all_content_blocks)

                # Get the user's message token count from the last message
                last_user_message = None
                for message in reversed(conversation.get_messages()):
                    if message["role"] == "user":
                        last_user_message = message["content"]
                        break

                user_tokens = (
                    count_tokens(last_user_message) if last_user_message else 0
                )

                # Count tokens in assistant's response
                assistant_tokens = count_tokens(assistant_response_text)

                # Count tokens in visible conversation (excluding thinking blocks)
                if hasattr(conversation, "get_visible_messages"):
                    visible_conversation_text = json.dumps(
                        conversation.get_visible_messages()
                    )
                    visible_conversation_tokens = count_tokens(
                        visible_conversation_text
                    )
                    overhead_tokens = (
                        visible_conversation_tokens - user_tokens - assistant_tokens
                    )
                    total_tokens = user_tokens + assistant_tokens + overhead_tokens
                else:
                    # Count tokens in entire conversation
                    conversation_text = json.dumps(conversation.get_messages())
                    conversation_tokens = count_tokens(conversation_text)
                    overhead_tokens = (
                        conversation_tokens - user_tokens - assistant_tokens
                    )
                    total_tokens = user_tokens + assistant_tokens + overhead_tokens

                # Display the total and breakdown
                console.print(
                    f"\n[cyan][Tokens: {total_tokens} total = {user_tokens} (user) + {assistant_tokens} (assistant) + {overhead_tokens} (overhead)][/cyan]"
                )

                # Display token usage if available
                if hasattr(stream, "usage") and stream.usage:
                    input_tokens = stream.usage.input_tokens
                    output_tokens = stream.usage.output_tokens
                    console.print(
                        f"[dim]API usage: {input_tokens} input, {output_tokens} output[/dim]"
                    )

        # Handle tool calls if any
        if is_tool_use:
            console.print("\n[bold yellow]Executing tools...[/bold yellow]")

            # Execute each tool call and add results to conversation
            for tool_call in tool_calls:
                logging.info(
                    f"Executing tool call: {tool_call['name']} (ID: {tool_call['id']})"
                )
                result = execute_tool_call(tool_call)
                logging.info(f"Tool result type: {type(result).__name__}")

                try:
                    # Log the result before converting to JSON
                    logging.debug(f"Raw tool result: {result}")
                    result_str = json.dumps(result)
                    logging.info(
                        f"Converted tool result to JSON string (length: {len(result_str)})"
                    )
                except Exception as e:
                    logging.error(f"Error converting tool result to JSON: {str(e)}")
                    logging.error(f"Exception type: {type(e).__name__}")
                    logging.error(f"Result that failed conversion: {result}")
                    # Provide a fallback result
                    result_str = json.dumps(
                        {"error": f"Failed to convert result: {str(e)}"}
                    )
                    logging.info("Using fallback error result")

                # FIXED: Only add the tool result without thinking blocks
                logging.info(
                    f"Adding tool result to conversation for ID: {tool_call['id']}"
                )
                conversation.add_tool_result(tool_call["id"], result_str)
                console.print(f"[dim]Tool result added to conversation[/dim]")

            # Continue the conversation with new information
            console.print(
                "\n[bold yellow]Continuing with tool results...[/bold yellow]"
            )

            # Check if we need to trim thinking tokens before continuing
            trim_thinking_tokens(conversation)

            # Process the conversation again
            process_conversation_without_thinking(client, conversation, show_thinking)

    except Exception as e:
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        logging.error(f"API error: {str(e)}")
        logging.error(f"Exception type: {type(e).__name__}")
        logging.error(f"Exception details:", exc_info=True)


if __name__ == "__main__":
    main()

================
File: config.py
================
import os
from dotenv import load_dotenv
from datetime import datetime

# Load environment variables from .env file
load_dotenv()

# Configuration settings
CONFIG = {
    "model": "claude-3-7-sonnet-20250219",
    "anthropic_api_key": os.environ.get("ANTHROPIC_API_KEY"),
    "max_tokens": 20000,
    "temperature": 1.0,
    # Extended thinking configuration
    "thinking": {
        "enabled": True,
        "budget_tokens": 16000,
    },
}

# Get current timestamp
current_timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

# System prompt
SYSTEM_PROMPT = f"""You are a helpful AI assistant named Rubra. 
You answer user questions thoughtfully and accurately using tools when necessary.
Current timestamp: {current_timestamp}

You have access to tools that can search the web, research specific websites, and manage documents in Notion.
ALWAYS use google_search for:
- Current events, news, or sports information (like "what's the NBA schedule today?")
- Facts that might have changed since your training
- Specific information that requires up-to-date data
- You may need to use google_search multiple times to get all the information you need and use web_research to explore the results.

ALWAYS use web_research when:
- A user specifically asks about content on a website (like "what are the headlines on cnbc.com?")
- You need information from a specific URL
- You need to verify information on a webpage

ALWAYS use notion_agent when:
- You want to create, edit, or manage documents in Notion
- A user asks about documents in Notion
- A user or you wants to add, modify, or remove sections in Notion documents
- A user or you wants to export documents or get summaries of Notion documents

IMPORTANT: When using notion_agent:
1. Send a SINGLE, COMPLETE instruction that includes all details about the document operation
2. Do NOT make multiple notion_agent calls for what should be a single document operation
3. Let the notion_agent handle all document management steps internally
4. Include all necessary details in your instruction (document titles, section names, content, etc.)

When you don't know something, use your tools rather than making up information.

IMPORTANT: When providing information from search results or web research, you MUST:
1. Use numbered citations like [1], [2], etc. after each fact or piece of information
2. Include a "References" section at the end of your response listing all sources
3. Format each reference with the source name and URL when available

Example citation format:
"NVIDIA stock closed at $130.28 today [1]."

Example references section:
"References:
[1] CNBC - https://www.cnbc.com/quotes/NVDA
[2] Yahoo Finance - https://finance.yahoo.com/quote/NVDA/"

IMPORTANT: When answering time-related questions (like "what time is it?"), use the timestamp information provided to you, but DO NOT mention or reference the timestamp in your response. Simply provide the time information naturally as if you know the current time.
"""


# Function to append timestamp to user messages in a hidden way
def append_timestamp_to_message(message):
    """Appends the current timestamp to a user message in a way that's not visible in the final output."""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    # Use a special format that won't be visible in the final output but will be processed by the model
    return f"{message}\n\n<timestamp>{timestamp}</timestamp>"


# Tool definitions
TOOLS = [
    {
        "name": "google_search",
        "description": "Performs a Google search with the specified query and returns results.",
        "input_schema": {
            "type": "object",
            "required": ["query"],
            "properties": {
                "query": {
                    "type": "string",
                    "description": "The search query string. Dont use too many terms.",
                }
            },
        },
    },
    {
        "name": "web_research",
        "description": "Researches a specific webpage to answer a question",
        "input_schema": {
            "type": "object",
            "required": ["url", "query"],
            "properties": {
                "url": {
                    "type": "string",
                    "description": "The URL of the webpage to research.",
                },
                "query": {
                    "type": "string",
                    "description": "The specific question to answer from the webpage.",
                },
            },
        },
    },
    {
        "name": "notion_agent",
        "description": "Manages all document operations in Notion. Send a single, clear instruction about what document operations you want performed (create, edit, manage documents). The Notion agent will handle all necessary steps internally, including getting document content, making changes, and validating structure. For links, use the link property within text objects. For tables, specify table_width and create table_row blocks with cells. ALWAYS include proper citations when adding factual information to documents, with a References section at the end of each document.",
        "input_schema": {
            "type": "object",
            "required": ["query"],
            "properties": {
                "query": {
                    "type": "string",
                    "description": "A complete, detailed instruction for Notion document operations. Include all necessary details in a single query rather than making multiple calls.",
                }
            },
        },
    },
]

================
File: conversation.py
================
import logging
import json


class Conversation:
    """Conversation history manager with tool call support."""

    def __init__(self):
        self.messages = []

    def add_user_message(self, content: str):
        """Add a user message to the conversation."""
        # Directly add the user message without any timestamp processing
        self.messages.append({"role": "user", "content": content})

    def add_assistant_message(self, content_blocks):
        """
        Add an assistant message to the conversation history.

        Args:
            content_blocks: List of content blocks from the assistant
        """
        # Extract text content and tool use from content blocks
        message = {"role": "assistant", "content": []}

        # Add each content block directly to the message content
        for block in content_blocks:
            if block["type"] == "text":
                message["content"].append(
                    {"type": "text", "text": block.get("text", "")}
                )
            elif block["type"] == "tool_use":
                # Log tool use details for debugging
                tool_name = block.get("name", "unknown_tool")
                tool_input = block.get("input", {})
                logging.debug(
                    f"Tool call detected - Name: {tool_name}, "
                    f"Parameters: {tool_input}"
                )

                message["content"].append(
                    {
                        "type": "tool_use",
                        "id": block.get("id"),
                        "name": block.get("name"),
                        "input": block.get("input"),
                    }
                )

        self.messages.append(message)

    def add_tool_result(self, tool_id, result):
        """
        Add a tool result to the conversation.

        Args:
            tool_id: The ID of the tool that was called
            result: The result of the tool execution (string or dict)
        """
        logging.info(f"Adding tool result for tool ID: {tool_id}")
        logging.info(f"Result type: {type(result).__name__}")

        # Get the last assistant message to preserve thinking blocks
        last_assistant_message = None
        for message in reversed(self.messages):
            if message["role"] == "assistant":
                last_assistant_message = message
                break

        logging.info(
            f"Found last assistant message: {last_assistant_message is not None}"
        )

        # Extract thinking blocks from the last assistant message
        thinking_blocks = []
        if last_assistant_message:
            for block in last_assistant_message.get("content", []):
                if block["type"] in ["thinking", "redacted_thinking"]:
                    thinking_blocks.append(block)

            logging.info(f"Extracted {len(thinking_blocks)} thinking blocks")

        # Create a new user message with tool results and preserved thinking blocks
        # Only include thinking blocks if there are any
        content_blocks = []
        if thinking_blocks:
            content_blocks.extend(thinking_blocks)
            logging.info(f"Added {len(thinking_blocks)} thinking blocks to content")

        # Ensure result is a string
        try:
            if isinstance(result, dict):
                logging.info("Converting dict result to JSON string")
                result_str = json.dumps(result)
            elif not isinstance(result, str):
                logging.info(f"Converting {type(result).__name__} result to string")
                result_str = str(result)
            else:
                logging.info("Result is already a string, using as is")
                result_str = result

            logging.debug(
                f"Final result string (first 100 chars): {result_str[:100]}..."
            )
        except Exception as e:
            logging.error(f"Error converting result to string: {str(e)}")
            logging.error(f"Exception type: {type(e).__name__}")
            # Provide a fallback
            result_str = json.dumps({"error": f"Failed to convert result: {str(e)}"})
            logging.info("Using fallback error result")

        content_blocks.append(
            {"type": "tool_result", "tool_use_id": tool_id, "content": result_str}
        )
        logging.info(f"Created tool result content block for tool ID: {tool_id}")

        tool_result_message = {
            "role": "user",
            "content": content_blocks,
        }

        self.messages.append(tool_result_message)
        logging.info("Added tool result message to conversation history")

    def get_messages(self):
        """Get all messages in the conversation."""
        return self.messages

    def clear(self):
        """Clear the conversation history."""
        self.messages = []

    def is_first_turn(self):
        """Check if this is the first conversation turn."""
        return getattr(self, "_first_turn", True)

    def mark_not_first_turn(self):
        """Mark that we're no longer in the first conversation turn."""
        self._first_turn = False

    def get_visible_messages(self):
        """Get messages without thinking blocks for token counting."""
        visible_messages = []
        for message in self.messages:
            if message["role"] == "user":
                # User messages are kept as is
                visible_messages.append(message)
            elif message["role"] == "assistant":
                # For assistant messages, filter out thinking blocks
                visible_content = []
                for block in message.get("content", []):
                    if block["type"] not in ["thinking", "redacted_thinking"]:
                        visible_content.append(block)

                if visible_content:
                    visible_messages.append(
                        {"role": "assistant", "content": visible_content}
                    )

        return visible_messages

================
File: requirements.txt
================
anthropic>=0.15.0
rich>=13.0.0
python-dotenv>=1.0.0

================
File: tools.py
================
import requests
import logging
import json
import os
from typing import Dict, Any


def google_search(q: str) -> Dict[str, Any]:
    """
    Performs a Google search via local microservice.

    Args:
        q: The search query string

    Returns:
        Dict containing search results or error
    """
    try:
        response = requests.get("http://localhost:8085/search", params={"q": q})
        response.raise_for_status()
        result = response.json()
        logging.info(
            f"Search for '{q}' returned {len(result.get('items', []))} results"
        )
        return result
    except Exception as e:
        logging.error(f"Search error: {e}")
        return {"error": str(e), "items": []}


def web_research(url: str, query: str) -> Dict[str, Any]:
    """
    Research a specific webpage to answer a question.
    Uses the web research microservice to do targeted extraction.

    Args:
        url: The URL to research
        query: The specific question to answer

    Returns:
        Dict containing the research result or error
    """
    try:
        response = requests.post(
            "http://localhost:8090/research", json={"url": url, "query": query}
        )
        response.raise_for_status()
        result = response.json()
        logging.info(f"Web research for '{query}' on {url} completed")
        return result
    except Exception as e:
        logging.error(f"Web research error: {e}")
        return {"error": str(e), "answer": f"Failed to research {url}: {str(e)}"}


def notion_agent(query: str) -> Dict[str, Any]:
    """
    Sends a natural language query to the Notion document agent.

    Args:
        query: Natural language instruction for Notion document operations

    Returns:
        Dict containing the response from the Notion agent
    """
    try:
        # Add specific formatting guidance with examples of proper Notion blocks
        enhanced_query = (
            f"{query}\n\n"
            "IMPORTANT: The content currently being created in Notion looks poorly formatted. "
            "Please ensure all content is properly formatted using Notion's native block structure. "
            "DO NOT use raw markdown or plain text with URLs. Instead:\n\n"
            "1. For headings, use proper heading blocks:\n"
            "   - heading_1 for main titles\n"
            "   - heading_2 for section headers\n"
            "   - heading_3 for subsections\n\n"
            "2. For links, embed them properly in text using the link property:\n"
            "   - Bad: 'Check website: https://example.com'\n"
            "   - Good: Text with the URL embedded as a proper link\n\n"
            "3. For lists, use proper bulleted_list_item or numbered_list_item blocks\n\n"
            "4. For tables, use proper table blocks with table_rows and cells\n\n"
            "5. For paragraphs, use paragraph blocks with properly formatted rich_text\n\n"
            "Example of a properly formatted paragraph with a link:\n"
            "{\n"
            '  "paragraph": {\n'
            '    "rich_text": [\n'
            "      {\n"
            '        "type": "text",\n'
            '        "text": {\n'
            '          "content": "Visit ",\n'
            '          "link": null\n'
            "        }\n"
            "      },\n"
            "      {\n"
            '        "type": "text",\n'
            '        "text": {\n'
            '          "content": "Basketball Reference",\n'
            '          "link": {\n'
            '            "url": "https://www.basketball-reference.com/players/j/jamesle01.html"\n'
            "          }\n"
            "        },\n"
            '        "annotations": {\n'
            '          "bold": true,\n'
            '          "color": "blue"\n'
            "        }\n"
            "      },\n"
            "      {\n"
            '        "type": "text",\n'
            '        "text": {\n'
            '          "content": " for LeBron James statistics.",\n'
            '          "link": null\n'
            "        }\n"
            "      }\n"
            "    ]\n"
            "  }\n"
            "}\n\n"
            "Example of a proper heading:\n"
            "{\n"
            '  "heading_2": {\n'
            '    "rich_text": [\n'
            "      {\n"
            '        "type": "text",\n'
            '        "text": {\n'
            '          "content": "LeBron James Career Statistics",\n'
            '          "link": null\n'
            "        }\n"
            "      }\n"
            "    ]\n"
            "  }\n"
            "}\n\n"
            "ENSURE all content is properly structured for Notion's native display."
        )

        # Log the query for debugging
        logging.info(f"Sending query to Notion agent: {enhanced_query}")

        response = requests.post(
            "http://localhost:8092/notion/query", json={"query": enhanced_query}
        )
        response.raise_for_status()
        result = response.json()
        logging.info(f"Notion agent response: {result}")

        # Extract the important information from the response
        correlation_id = result.get("correlation_id", "")
        status = result.get("status", "")
        message = result.get("message", "")
        tool_results = result.get("tool_results", [])

        # Create a summary of operations performed
        operations_summary = ""
        if tool_results:
            operations_summary = "\n\nOperations performed:\n"
            for i, tool_result in enumerate(tool_results, 1):
                tool_name = tool_result.get("tool", "unknown")
                tool_input = tool_result.get("input", {})
                result_data = tool_result.get("result", {})

                # Extract the most relevant information based on the tool type
                if tool_name == "create_document" and result_data.get("success"):
                    page_id = result_data.get("page_id", "")
                    title = result_data.get("title", "")
                    url = result_data.get("url", "")
                    operations_summary += f"{i}. Created document: '{title}' (ID: {page_id})\n   URL: {url}\n"

                elif tool_name == "get_document" and result_data.get("success"):
                    title = result_data.get("title", "")
                    operations_summary += f"{i}. Retrieved document: '{title}'\n"

                elif tool_name == "search_documents" and result_data.get("success"):
                    documents = result_data.get("documents", [])
                    doc_count = len(documents)
                    operations_summary += f"{i}. Found {doc_count} documents\n"
                    # List the first few documents
                    for j, doc in enumerate(documents[:5], 1):
                        doc_title = doc.get("title", "")
                        doc_id = doc.get("page_id", "")
                        operations_summary += f"   {j}. '{doc_title}' (ID: {doc_id})\n"
                    if doc_count > 5:
                        operations_summary += f"   ... and {doc_count - 5} more\n"

                else:
                    # Generic handling for other tool types
                    success = result_data.get("success", False)
                    result_msg = result_data.get("message", "No details")
                    operations_summary += f"{i}. {tool_name}: {result_msg}\n"

        # Include the raw response for debugging if needed
        return {
            "status": status,
            "message": message + operations_summary,
            "correlation_id": correlation_id,
            "raw_response": result,
        }
    except Exception as e:
        error_msg = f"Notion agent error: {str(e)}"
        logging.error(error_msg)
        return {
            "error": error_msg,
            "message": f"Failed to process Notion document operation: {str(e)}",
        }


# Document manipulation functions
def _ensure_workspace_dir():
    """Ensures the workspace directory exists."""
    workspace_dir = "./workspace"
    if not os.path.exists(workspace_dir):
        os.makedirs(workspace_dir)
    return workspace_dir


def _read_file_lines(file_path):
    """Utility to read file lines safely."""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            return f.readlines()
    except Exception as e:
        logging.error(f"Error reading file {file_path}: {str(e)}")
        return []


def _write_file_lines(file_path, lines):
    """Utility to write lines to file safely."""
    try:
        with open(file_path, "w", encoding="utf-8") as f:
            f.writelines(lines)
        return True
    except Exception as e:
        logging.error(f"Error writing to file {file_path}: {str(e)}")
        return False


def create_document(file_name: str, text_content: str = "") -> Dict[str, Any]:
    """
    Creates or overwrites a Markdown research document.

    Args:
        file_name: Name of the markdown file
        text_content: Optional initial text to place in the document

    Returns:
        Dict containing status information
    """
    try:
        workspace_dir = _ensure_workspace_dir()
        file_path = os.path.join(workspace_dir, file_name)

        with open(file_path, "w", encoding="utf-8") as f:
            f.write(text_content)

        logging.info(f"Document created: {file_name}")
        return {"status": "created", "file": file_name}
    except Exception as e:
        error_msg = f"Failed to create document {file_name}: {str(e)}"
        logging.error(error_msg)
        return {"error": error_msg}


def add_section(
    file_name: str, section_name: str, text_content: str = ""
) -> Dict[str, Any]:
    """
    Adds a new section to an existing markdown file.

    Args:
        file_name: The markdown file to modify
        section_name: The name/title of the new section to create
        text_content: Optional text content to include below the heading

    Returns:
        Dict containing status information
    """
    try:
        workspace_dir = _ensure_workspace_dir()
        file_path = os.path.join(workspace_dir, file_name)

        if not os.path.isfile(file_path):
            return {"error": f"File '{file_name}' does not exist."}

        lines = _read_file_lines(file_path)
        heading_str = f"# {section_name}"

        if any(heading_str in line for line in lines):
            return {
                "error": f"Section '{section_name}' already exists in '{file_name}'."
            }

        with open(file_path, "a", encoding="utf-8") as f:
            f.write(f"\n# {section_name}\n{text_content}\n")

        logging.info(f"Section '{section_name}' added to {file_name}")
        return {"status": "section_added", "section_name": section_name}
    except Exception as e:
        error_msg = f"Failed to add section to {file_name}: {str(e)}"
        logging.error(error_msg)
        return {"error": error_msg}


def append_block(
    file_name: str, section_name: str, text_content: str
) -> Dict[str, Any]:
    """
    Appends additional text to an existing section in a markdown file.

    Args:
        file_name: The markdown file to modify
        section_name: The name of the existing section to append to
        text_content: The text block to add at the end of that section

    Returns:
        Dict containing status information
    """
    try:
        workspace_dir = _ensure_workspace_dir()
        file_path = os.path.join(workspace_dir, file_name)

        if not os.path.isfile(file_path):
            return {"error": f"File '{file_name}' does not exist."}

        lines = _read_file_lines(file_path)
        heading_variants = [
            f"# {section_name}",
            f"## {section_name}",
            f"### {section_name}",
        ]

        # Check if section exists
        if not any(any(hv in line for hv in heading_variants) for line in lines):
            return {"error": f"Section '{section_name}' not found in '{file_name}'."}

        # For simplicity, we'll just append to the end of the file
        # A more sophisticated implementation would find the exact section
        with open(file_path, "a", encoding="utf-8") as f:
            f.write(f"\n{text_content}\n")

        logging.info(f"Text appended to section '{section_name}' in {file_name}")
        return {"status": "block_appended", "section": section_name}
    except Exception as e:
        error_msg = f"Failed to append block to {file_name}: {str(e)}"
        logging.error(error_msg)
        return {"error": error_msg}


def replace_section(
    file_name: str, section_name: str, text_content: str
) -> Dict[str, Any]:
    """
    Overwrites the entire content of a named section with new text.

    Args:
        file_name: The markdown file to modify
        section_name: Name of the section to replace
        text_content: The text that replaces everything in that section

    Returns:
        Dict containing status information
    """
    try:
        workspace_dir = _ensure_workspace_dir()
        file_path = os.path.join(workspace_dir, file_name)

        if not os.path.isfile(file_path):
            return {"error": f"File '{file_name}' does not exist."}

        lines = _read_file_lines(file_path)
        heading_variants = [
            f"# {section_name}",
            f"## {section_name}",
            f"### {section_name}",
            f"#### {section_name}",
        ]

        new_lines = []
        inside_target = False
        replaced = False

        i = 0
        while i < len(lines):
            line = lines[i]
            if any(line.strip().startswith(hv) for hv in heading_variants):
                # Start of our section
                inside_target = True
                replaced = True
                # Insert heading
                new_lines.append(line)
                # Insert the replacement text
                new_lines.append(text_content + "\n")
                # Skip lines until next heading
                i += 1
                while i < len(lines):
                    # if we see another heading, break
                    if lines[i].startswith("#"):
                        break
                    i += 1
                # Don't increment i again here, so we check the next line in the loop
                continue
            else:
                new_lines.append(line)
            i += 1

        if not replaced:
            # If we didn't find that heading, create it at the end
            new_lines.append(f"\n# {section_name}\n{text_content}\n")

        _write_file_lines(file_path, new_lines)

        logging.info(f"Section '{section_name}' replaced in {file_name}")
        return {"status": "section_replaced", "section_name": section_name}
    except Exception as e:
        error_msg = f"Failed to replace section in {file_name}: {str(e)}"
        logging.error(error_msg)
        return {"error": error_msg}


def remove_section(file_name: str, section_name: str) -> Dict[str, Any]:
    """
    Removes an entire section by name from the markdown file.

    Args:
        file_name: The markdown file to modify
        section_name: Name of the section to remove

    Returns:
        Dict containing status information
    """
    try:
        workspace_dir = _ensure_workspace_dir()
        file_path = os.path.join(workspace_dir, file_name)

        if not os.path.isfile(file_path):
            return {"error": f"File '{file_name}' does not exist."}

        lines = _read_file_lines(file_path)
        heading_variants = [
            f"# {section_name}",
            f"## {section_name}",
            f"### {section_name}",
            f"#### {section_name}",
        ]

        new_lines = []
        removed = False

        i = 0
        while i < len(lines):
            line = lines[i]
            # If we see the section heading, skip until next heading
            if any(line.strip().startswith(hv) for hv in heading_variants):
                removed = True
                i += 1
                while i < len(lines):
                    if lines[i].startswith("#"):
                        break
                    i += 1
            else:
                new_lines.append(line)
                i += 1

        if not removed:
            return {"error": f"Section '{section_name}' not found in '{file_name}'."}

        _write_file_lines(file_path, new_lines)

        logging.info(f"Section '{section_name}' removed from {file_name}")
        return {"status": "section_removed", "section_name": section_name}
    except Exception as e:
        error_msg = f"Failed to remove section from {file_name}: {str(e)}"
        logging.error(error_msg)
        return {"error": error_msg}


def rename_section(
    file_name: str, section_name: str, text_content: str
) -> Dict[str, Any]:
    """
    Renames an existing section heading in the markdown file.

    Args:
        file_name: The markdown file to modify
        section_name: Existing section name to rename
        text_content: The new name for this section heading

    Returns:
        Dict containing status information
    """
    try:
        workspace_dir = _ensure_workspace_dir()
        file_path = os.path.join(workspace_dir, file_name)

        if not os.path.isfile(file_path):
            return {"error": f"File '{file_name}' does not exist."}

        lines = _read_file_lines(file_path)
        heading_variants = [
            f"# {section_name}",
            f"## {section_name}",
            f"### {section_name}",
            f"#### {section_name}",
        ]

        new_lines = []
        renamed = False

        for line in lines:
            stripped_line = line.strip()
            if any(stripped_line.startswith(hv) for hv in heading_variants):
                # rename the heading
                # keep the same number of #, just rename the text
                prefix = stripped_line.split(section_name)[0]  # e.g. "# " or "## "
                new_line = prefix + text_content + "\n"
                new_lines.append(new_line)
                renamed = True
            else:
                new_lines.append(line)

        if not renamed:
            return {"error": f"Section '{section_name}' not found in '{file_name}'."}

        _write_file_lines(file_path, new_lines)

        logging.info(
            f"Section '{section_name}' renamed to '{text_content}' in {file_name}"
        )
        return {
            "status": "section_renamed",
            "old_section": section_name,
            "new_section": text_content,
        }
    except Exception as e:
        error_msg = f"Failed to rename section in {file_name}: {str(e)}"
        logging.error(error_msg)
        return {"error": error_msg}


def export_document(file_name: str, export_format: str) -> Dict[str, Any]:
    """
    Converts an existing markdown file to a specified format.

    Args:
        file_name: The markdown file to export
        export_format: The desired output format (docx, pdf, md, html)

    Returns:
        Dict containing status information
    """
    try:
        workspace_dir = _ensure_workspace_dir()
        file_path = os.path.join(workspace_dir, file_name)

        if not os.path.isfile(file_path):
            return {"error": f"File '{file_name}' does not exist."}

        # In a real implementation, you would use a library like Pandoc to convert
        # For now, we'll just simulate the export

        output_file = os.path.splitext(file_name)[0] + "." + export_format
        output_path = os.path.join(workspace_dir, output_file)

        # Simulate export by copying the file (in a real implementation, use proper conversion)
        with open(file_path, "r", encoding="utf-8") as src:
            content = src.read()

        with open(output_path, "w", encoding="utf-8") as dest:
            dest.write(content)

        logging.info(f"Document '{file_name}' exported to {output_file}")
        return {
            "status": "exported",
            "original_file": file_name,
            "export_format": export_format,
            "output_file": output_file,
        }
    except Exception as e:
        error_msg = f"Failed to export document {file_name}: {str(e)}"
        logging.error(error_msg)
        return {"error": error_msg}
